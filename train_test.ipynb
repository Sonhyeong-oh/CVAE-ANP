{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨"
      ],
      "metadata": {
        "id": "xheR6WNDju6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def remove_extreme_outliers(file_path, save_path=None):\n",
        "    print(f\"üìÇ ÌååÏùº Î°úÎî© Ï§ë: {file_path}\")\n",
        "    df = pd.read_excel(file_path)\n",
        "    original_len = len(df)\n",
        "\n",
        "    # 1. Í≤ÄÏÇ¨Ìï† Ïª¨Îüº ÏßÄÏ†ï (Ïò®ÎèÑ ÏÑºÏÑúÎì§)\n",
        "    # Î¨∏Ï†úÍ∞Ä ÎêòÏóàÎçò VFD Ïò®ÎèÑ ÏÑºÏÑúÎì§Îßå ÌÉÄÍ≤üÏúºÎ°ú Ïû°ÏäµÎãàÎã§.\n",
        "    target_cols = [\n",
        "        'Q_VFD1_Temperature',\n",
        "        'Q_VFD2_Temperature',\n",
        "        'Q_VFD3_Temperature',\n",
        "        'Q_VFD4_Temperature'\n",
        "    ]\n",
        "\n",
        "    # Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî Ïª¨ÎüºÎßå ÌïÑÌÑ∞ÎßÅ\n",
        "    exist_cols = [col for col in target_cols if col in df.columns]\n",
        "\n",
        "    if not exist_cols:\n",
        "        print(\"‚ö†Ô∏è ÌÉÄÍ≤ü Ïª¨Îüº(Ïò®ÎèÑ ÏÑºÏÑú)Ïù¥ Îç∞Ïù¥ÌÑ∞Ïóê ÏóÜÏäµÎãàÎã§. Ïä§ÌÇµÌï©ÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    # 2. ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï\n",
        "    MAX_THRESHOLD = 10000    # 9Ïñµ Í∞ôÏùÄ ÎÑàÎ¨¥ ÌÅ∞ Í∞í Ï†úÍ±∞\n",
        "    MIN_THRESHOLD = 0.001    # 1e-38 Í∞ôÏùÄ ÎÑàÎ¨¥ ÏûëÏùÄ Í∞í(0 Ìè¨Ìï®) Ï†úÍ±∞\n",
        "\n",
        "    # 3. ÏÇ≠Ï†ú Î°úÏßÅ (ÌïòÎÇòÎùºÎèÑ Î≤îÏúÑÎ•º Î≤óÏñ¥ÎÇòÎ©¥ Í∑∏ Ìñâ ÏÇ≠Ï†ú)\n",
        "    # Ï°∞Í±¥: (Í∞íÏù¥ 10000 Ï¥àÍ≥º) OR (Í∞íÏù¥ 0.001 ÎØ∏Îßå)\n",
        "    # Ï£ºÏùò: ÌåêÎã§Ïä§ÏóêÏÑúÎäî 'or' ÎåÄÏã† '|' Ïó∞ÏÇ∞ÏûêÎ•º Ïç®Ïïº ÌïòÎ©∞, Í¥ÑÌò∏Î°ú Î¨∂Ïñ¥Ïïº Ìï©ÎãàÎã§.\n",
        "    outlier_mask = ((df[exist_cols] > MAX_THRESHOLD) | (df[exist_cols] < MIN_THRESHOLD)).any(axis=1)\n",
        "\n",
        "    # Ï†ïÏÉÅ Îç∞Ïù¥ÌÑ∞Îßå ÎÇ®Í∏∞Í∏∞ (~outlier_mask)\n",
        "    clean_df = df[~outlier_mask]\n",
        "\n",
        "    # 4. Í≤∞Í≥º Ï∂úÎ†•\n",
        "    removed_count = original_len - len(clean_df)\n",
        "    print(f\"   - Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞: {original_len}Í∞ú\")\n",
        "    print(f\"   - ÏÇ≠Ï†úÎêú Ïù¥ÏÉÅÏπò Ìñâ: {removed_count}Í∞ú\")\n",
        "    print(f\"     (ÏÇ≠Ï†ú Ï°∞Í±¥: Í∞í > {MAX_THRESHOLD} ÎòêÎäî Í∞í < {MIN_THRESHOLD})\")\n",
        "    print(f\"   - ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞: {len(clean_df)}Í∞ú\")\n",
        "\n",
        "    # 5. ÏóëÏÖÄ Ï†ÄÏû•\n",
        "    if save_path is None:\n",
        "        save_path = file_path.replace(\".xlsx\", \"_cleaned.xlsx\")\n",
        "\n",
        "    print(f\"üíæ Ï†ÄÏû• Ï§ë... -> {save_path}\")\n",
        "    clean_df.to_excel(save_path, index=False)\n",
        "    print(\"‚úÖ ÏôÑÎ£å!\\n\")\n",
        "\n",
        "# --- Ïã§Ìñâ Î∂ÄÎ∂Ñ ---\n",
        "train_file = \"/content/preproc_train_data_final.xlsx\"\n",
        "val_file = \"/content/preproc_val_data_final.xlsx\"\n",
        "test_file = \"/content/preproc_test_data_final.xlsx\"\n",
        "\n",
        "if os.path.exists(train_file): remove_extreme_outliers(train_file)\n",
        "else: print(f\"‚ùå ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {train_file}\")\n",
        "\n",
        "if os.path.exists(val_file): remove_extreme_outliers(val_file)\n",
        "else: print(f\"‚ùå ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {val_file}\")\n",
        "\n",
        "if os.path.exists(test_file): remove_extreme_outliers(test_file)\n",
        "else: print(f\"‚ùå ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {test_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtvg7TAO3_mY",
        "outputId": "dcec59a4-beba-4794-d80a-df67401ab365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ ÌååÏùº Î°úÎî© Ï§ë: /content/preproc_train_data_final.xlsx\n",
            "   - Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞: 80088Í∞ú\n",
            "   - ÏÇ≠Ï†úÎêú Ïù¥ÏÉÅÏπò Ìñâ: 1253Í∞ú\n",
            "     (ÏÇ≠Ï†ú Ï°∞Í±¥: Í∞í > 10000 ÎòêÎäî Í∞í < 0.001)\n",
            "   - ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞: 78835Í∞ú\n",
            "üíæ Ï†ÄÏû• Ï§ë... -> /content/preproc_train_data_final_cleaned.xlsx\n",
            "‚úÖ ÏôÑÎ£å!\n",
            "\n",
            "üìÇ ÌååÏùº Î°úÎî© Ï§ë: /content/preproc_val_data_final.xlsx\n",
            "   - Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞: 12919Í∞ú\n",
            "   - ÏÇ≠Ï†úÎêú Ïù¥ÏÉÅÏπò Ìñâ: 205Í∞ú\n",
            "     (ÏÇ≠Ï†ú Ï°∞Í±¥: Í∞í > 10000 ÎòêÎäî Í∞í < 0.001)\n",
            "   - ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞: 12714Í∞ú\n",
            "üíæ Ï†ÄÏû• Ï§ë... -> /content/preproc_val_data_final_cleaned.xlsx\n",
            "‚úÖ ÏôÑÎ£å!\n",
            "\n",
            "üìÇ ÌååÏùº Î°úÎî© Ï§ë: /content/preproc_test_data_final.xlsx\n",
            "   - Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞: 12949Í∞ú\n",
            "   - ÏÇ≠Ï†úÎêú Ïù¥ÏÉÅÏπò Ìñâ: 206Í∞ú\n",
            "     (ÏÇ≠Ï†ú Ï°∞Í±¥: Í∞í > 10000 ÎòêÎäî Í∞í < 0.001)\n",
            "   - ÎÇ®ÏùÄ Îç∞Ïù¥ÌÑ∞: 12743Í∞ú\n",
            "üíæ Ï†ÄÏû• Ï§ë... -> /content/preproc_test_data_final_cleaned.xlsx\n",
            "‚úÖ ÏôÑÎ£å!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CVAE-ANP ÌõàÎ†®"
      ],
      "metadata": {
        "id": "-AjS22lbcKV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏"
      ],
      "metadata": {
        "id": "QDPMR5N9cWGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyVNEDvXcHpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0adfecce-7738-404c-c85b-e755a56685a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1. Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Îç∞Ïù¥ÌÑ∞ ÏïïÏ∂ï Ìï¥Ï†ú"
      ],
      "metadata": {
        "id": "gnZ5r3-dcXaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Îç∞Ïù¥ÌÑ∞ ÏïïÏ∂ï ÌååÏùº Ìï¥Ï†ú\n",
        "import tarfile # zipfile ÎåÄÏã† tarfile ÏûÑÌè¨Ìä∏\n",
        "import os\n",
        "\n",
        "def unzip_to_content(zip_path, out_dir=\"/content\"):\n",
        "    assert os.path.exists(zip_path), f\"File not found: {zip_path}\"\n",
        "    print(f\"[EXTRACTING] {zip_path} ‚Üí {out_dir}\")\n",
        "\n",
        "    # ÌååÏùº ÌôïÏû•ÏûêÏóê Îî∞Îùº Îã§Î•∏ Î™®Îìà ÏÇ¨Ïö©\n",
        "    if zip_path.lower().endswith(('.zip')):\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            zf.extractall(out_dir)\n",
        "    elif zip_path.lower().endswith(('.tar', '.gz', '.tgz', '.bz2')):\n",
        "        with tarfile.open(zip_path, 'r') as tf:\n",
        "            tf.extractall(out_dir)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported archive format: {zip_path}\")\n",
        "    print(\"[DONE]\")\n",
        "\n",
        "# ===== ÏïïÏ∂ï ÌååÏùº Í≤ΩÎ°ú =====\n",
        "ZIP_FILES = [\n",
        "    \"/content/drive/MyDrive/FF_Dataset/Final.tar\"\n",
        "]\n",
        "\n",
        "for zp in ZIP_FILES:\n",
        "    unzip_to_content(zp)\n"
      ],
      "metadata": {
        "id": "qyTpiQb6cSNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6fd20f5-aa70-466e-ceae-3def9b8b6455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EXTRACTING] /content/drive/MyDrive/FF_Dataset/Final.tar ‚Üí /content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1473144184.py:15: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(out_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "fC1AcirHcY9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# 'VAE_ANP' Ìè¥ÎçîÍ∞Ä ÏûàÎäî Î∂ÄÎ™® ÎîîÎ†âÌÜ†Î¶¨Ïùò Í≤ΩÎ°úÎ•º ÏßÄÏ†ïÌïòÏÑ∏Ïöî.\n",
        "# Ïòà: ÎßåÏïΩ VAE_ANP Ìè¥ÎçîÍ∞Ä 'ÎÇ¥ ÎìúÎùºÏù¥Î∏å/my_project/VAE_ANP'Ïóê ÏûàÎã§Î©¥,\n",
        "# project_root_path = '/content/drive/My Drive/my_project'\n",
        "project_root_path = '/content/drive/My Drive/VAE_ANP/' # VAE_ANP Ìè¥Îçî ÏûêÏ≤¥Í∞Ä sys.pathÏóê Ï∂îÍ∞ÄÎêòÎèÑÎ°ù ÏàòÏ†ï\n",
        "\n",
        "# sys.pathÏóê Í≤ΩÎ°ú Ï∂îÍ∞Ä\n",
        "if project_root_path not in sys.path:\n",
        "    sys.path.append(project_root_path)\n",
        "    print(f\"'{project_root_path}' Í≤ΩÎ°úÍ∞Ä sys.pathÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§.\")\n",
        "else:\n",
        "    print(f\"'{project_root_path}' Í≤ΩÎ°úÎäî Ïù¥ÎØ∏ sys.pathÏóê ÏûàÏäµÎãàÎã§.\")\n",
        "\n",
        "# Ï∂îÍ∞ÄÎêú Í≤ΩÎ°ú ÌôïÏù∏ (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n",
        "print(\"Current sys.path:\")\n",
        "for p in sys.path:\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "OMTdxcOjcTaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f354a4c7-3876-4e7c-b71e-11410b01bad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/VAE_ANP/' Í≤ΩÎ°úÍ∞Ä sys.pathÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§.\n",
            "Current sys.path:\n",
            "/content\n",
            "/env/python\n",
            "/usr/lib/python312.zip\n",
            "/usr/lib/python3.12\n",
            "/usr/lib/python3.12/lib-dynload\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/extensions\n",
            "/root/.ipython\n",
            "/content/drive/My Drive/VAE_ANP/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò"
      ],
      "metadata": {
        "id": "AM5SHOH7cb6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "lkAGihmJcU2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05f8cd1-0d63-4c11-9837-a3570595cf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.2)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Requirement already satisfied: torchmetrics>0.7.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (26.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (0.15.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.7.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.3)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (26.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ ÏÑ§Ï†ï"
      ],
      "metadata": {
        "id": "dCk5IGKScdD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import os\n",
        "import ast\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import argparse\n",
        "\n",
        "# Î™®Îç∏ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
        "# from combined_model_log import AttentiveCVAEANP\n",
        "from ANP.anp_attn_log import NeuralProcess\n",
        "from VAE.cvae_v2_fix import ConditionalVAE_v2\n",
        "\n",
        "# ÌÅ¥ÎûòÏä§ Ïû¨Ï†ïÏùò (ÏàòÏ†ïÎ≥∏)\n",
        "class AttentiveCVAEANP(nn.Module):\n",
        "    def __init__(self, cvae_latent_dim, condition_dim, x_dim, hidden_dim, anp_latent_dim,\n",
        "                 img_size=256, beta=1.0, **kwargs): # [ÏàòÏ†ï] **kwargs Ï∂îÍ∞Ä\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "        # 1. CVAE (Í∏∞Ï°¥ ÎèôÏùº)\n",
        "        self.cvae = ConditionalVAE_v2(in_channels=3,\n",
        "                                      condition_dim=condition_dim,\n",
        "                                      latent_dim=cvae_latent_dim,\n",
        "                                      img_size=img_size)\n",
        "\n",
        "        image_feature_dim = self.cvae.hidden_dims[-1]\n",
        "\n",
        "        # 2. ANP (ÏàòÏ†ïÎê®)\n",
        "        # Ïó¨Í∏∞ÏÑú **kwargsÎ•º ÌÜµÌï¥ min_std, dropout Îì±ÏùÑ NeuralProcessÎ°ú Ï†ÑÎã¨Ìï©ÎãàÎã§.\n",
        "        self.anp = NeuralProcess(x_dim=x_dim,\n",
        "                                 y_dim=1,\n",
        "                                 hidden_dim=hidden_dim,\n",
        "                                 latent_dim=anp_latent_dim,\n",
        "                                 latent_enc_self_attn_type=\"ptmultihead\",\n",
        "                                 det_enc_self_attn_type=\"ptmultihead\",\n",
        "                                 det_enc_cross_attn_type=\"ptmultihead\",\n",
        "                                 image_attention_type=\"ptmultihead\",\n",
        "                                 use_self_attn=True,\n",
        "                                 global_context_dim=cvae_latent_dim,\n",
        "                                 image_feature_dim=image_feature_dim,\n",
        "                                 **kwargs) # [ÌïµÏã¨] ÎÇòÎ®∏ÏßÄ Ïù∏ÏûêÎì§(min_std, dropout)ÏùÑ ANPÎ°ú Ï†ÑÎã¨!\n",
        "\n",
        "    def forward(self, image, condition, context_x, context_y, target_x, target_y=None):\n",
        "        # Forward Î°úÏßÅÏùÄ Í∏∞Ï°¥Í≥º 100% ÎèôÏùº\n",
        "        cvae_output = self.cvae(image, condition=condition)\n",
        "        img_mu = cvae_output[2]\n",
        "        features = cvae_output[4]\n",
        "\n",
        "        cvae_loss_dict = self.cvae.loss_function(*cvae_output[:4])\n",
        "\n",
        "        if 'Reconstruction_Loss' in cvae_loss_dict and 'KLD' in cvae_loss_dict:\n",
        "            cvae_loss_dict['loss'] = cvae_loss_dict['Reconstruction_Loss'] + self.beta * cvae_loss_dict['KLD']\n",
        "\n",
        "        _, anp_loss_dict, anp_misc_outputs, latent_self_attn, image_cross_attn, det_attn_weights = self.anp(\n",
        "            context_x,\n",
        "            context_y,\n",
        "            target_x,\n",
        "            target_y,\n",
        "            global_context=img_mu,\n",
        "            image_features=features\n",
        "        )\n",
        "\n",
        "        return anp_misc_outputs['y_dist'], anp_loss_dict, cvae_loss_dict, latent_self_attn, image_cross_attn, det_attn_weights, img_mu\n",
        "\n",
        "# --- Ï†ÑÏó≠ ÏÉÅÏàò Î∞è ÏÑ§Ï†ï ---\n",
        "IMG_DIRS = [\n",
        "    \"/content/Final/Image/train/BATCH1000\", \"/content/Final/Image/train/BATCH2000\",\n",
        "    \"/content/Final/Image/train/BATCH3000\", \"/content/Final/Image/train/BATCH4000\",\n",
        "    \"/content/Final/Image/train/BATCH5000\", \"/content/Final/Image/train/BATCH6000\",\n",
        "    \"/content/Final/Image/train/BATCH7000\", \"/content/Final/Image/train/BATCH8000\",\n",
        "    \"/content/Final/Image/train/BATCH9000\", \"/content/Final/Image/train/BATCH10000\",\n",
        "    \"/content/Final/Image/train/BATCH11000\", \"/content/Final/Image/train/BATCH12000\",\n",
        "    \"/content/Final/Image/train/BATCH13000\", \"/content/Final/Image/train/BATCH14000\",\n",
        "    \"/content/Final/Image/train/BATCH15000\", \"/content/Final/Image/train/BATCH16000\",\n",
        "    \"/content/Final/Image/train/BATCH17000\", \"/content/Final/Image/train/BATCH18000\",\n",
        "    \"/content/Final/Image/train/BATCH19000\", \"/content/Final/Image/val/BATCH20000\",\n",
        "    \"/content/Final/Image/val/BATCH21000\", \"/content/Final/Image/val/BATCH22000\"\n",
        "]\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMAGE_RESOLUTION = 256\n",
        "ORIGINAL_IMG_DIMS = (1080, 720)\n",
        "X_DIM = 2\n",
        "BEST_MODEL_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/best_cvae_anp_model.pth\"\n",
        "SCALER_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/anp_scaler.joblib\"\n",
        "BEST_ATTENTION_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/best_attention_weights.pt\"\n",
        "TRAIN_DATA_PATH = \"/content/preproc_train_data_final.xlsx\"\n",
        "VAL_DATA_PATH = \"/content/preproc_val_data_final.xlsx\"\n",
        "PREPROC_DATA_SAVE_PATH = \"/content/preproc_data.xlsx\"\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train CVAE-ANP Model\")\n",
        "    parser.add_argument('--clip', type=float, default=1.0, help='Gradient clipping value.')\n",
        "    parser.add_argument('--anp_loss_weight', type=float, default=1.0, help='Weight for ANP loss.')\n",
        "    parser.add_argument('--cvae_loss_weight', type=float, default=1.0, help='Weight for CVAE loss.')\n",
        "    return parser.parse_args()\n",
        "\n",
        "# --- 2D Ï¢åÌëúÍ≥Ñ ÏÑ§Ï†ï ---\n",
        "COMPONENT_ORDER = ['MHS', 'R01', 'R02', 'R03', 'R04', 'Conv1', 'Conv2', 'Conv3', 'Conv4']\n",
        "\n",
        "COMPONENT_COORDS_0 = {\n",
        "    'MHS': {'h': 53, 'w': 183, 'x1': 232, 'x2': 415, 'y1': 182, 'y2': 235},\n",
        "    'R01': {'h': 160, 'w': 99, 'x1': 385, 'x2': 484, 'y1': 102, 'y2': 262},\n",
        "    'R02': {'h': 148, 'w': 140, 'x1': 273, 'x2': 413, 'y1': 143, 'y2': 291},\n",
        "    'R03': {'h': 187, 'w': 173, 'x1': 372, 'x2': 545, 'y1': 161, 'y2': 348},\n",
        "    'R04': {'h': 139, 'w': 131, 'x1': 173, 'x2': 304, 'y1': 145, 'y2': 284},\n",
        "    'Conv1': {'h': 158, 'w': 334, 'x1': 414, 'x2': 748, 'y1': 362, 'y2': 520},\n",
        "    'Conv2': {'h': 139, 'w': 346, 'x1': 380, 'x2': 726, 'y1': 210, 'y2': 349},\n",
        "    'Conv3': {'h': 43, 'w': 225, 'x1': 131, 'x2': 356, 'y1': 228, 'y2': 271},\n",
        "    'Conv4': {'h': 294, 'w': 380, 'x1': 91, 'x2': 471, 'y1': 268, 'y2': 562}\n",
        "}\n",
        "COMPONENT_COORDS_1 = {\n",
        "    'MHS': {'h': 251, 'w': 397, 'x1': 363, 'x2': 760, 'y1': 278, 'y2': 529},\n",
        "    'R01': {'h': 439, 'w': 227, 'x1': 199, 'x2': 426, 'y1': 124, 'y2': 563},\n",
        "    'R02': {'h': 187, 'w': 128, 'x1': 379, 'x2': 507, 'y1': 108, 'y2': 295},\n",
        "    'R03': {'h': 148, 'w': 120, 'x1': 287, 'x2': 407, 'y1': 106, 'y2': 254},\n",
        "    'R04': {'h': 264, 'w': 268, 'x1': 589, 'x2': 857, 'y1': 164, 'y2': 428},\n",
        "    'Conv1': {'h': 39, 'w': 183, 'x1': 179, 'x2': 362, 'y1': 178, 'y2': 217},\n",
        "    'Conv2': {'h': 182, 'w': 301, 'x1': 168, 'x2': 469, 'y1': 208, 'y2': 390},\n",
        "    'Conv3': {'h': 81, 'w': 215, 'x1': 457, 'x2': 672, 'y1': 264, 'y2': 345},\n",
        "    'Conv4': {'h': 106, 'w': 338, 'x1': 354, 'x2': 692, 'y1': 173, 'y2': 279}\n",
        "}"
      ],
      "metadata": {
        "id": "RWOkVrCgciB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ìï®Ïàò ÏßÄÏ†ï"
      ],
      "metadata": {
        "id": "AoY_hPM9cit4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sensor_to_component(sensor_name):\n",
        "    for component in sorted(COMPONENT_ORDER, key=len, reverse=True):\n",
        "        if component in sensor_name: return component\n",
        "    return 'Conv' if 'Conv' in sensor_name else None\n",
        "\n",
        "def create_sensor_coord_tensor(sensor_keys, coord_type):\n",
        "    coords_map = COMPONENT_COORDS_1 if coord_type == 1 else COMPONENT_COORDS_0\n",
        "    coords = []\n",
        "    w, h = ORIGINAL_IMG_DIMS\n",
        "    for key in sensor_keys:\n",
        "        component_name = map_sensor_to_component(key)\n",
        "        if component_name and component_name in coords_map:\n",
        "            box = coords_map[component_name]\n",
        "            coords.append([(box['x1'] + box['x2']) / 2.0 / w, (box['y1'] + box['y2']) / 2.0 / h])\n",
        "        else:\n",
        "            coords.append([0.0, 0.0]) # Îß§Ìïë Î∂àÍ∞Ä Ïãú Í∏∞Î≥∏Í∞í\n",
        "    return torch.tensor(coords, dtype=torch.float32)\n",
        "\n",
        "# --- [ÏàòÏ†ï] Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ ---\n",
        "class CombinedANPDataset(Dataset):\n",
        "    def __init__(self, features_df, mask_df, image_paths_series, labels_series,\n",
        "                 coord_tensor_0, coord_tensor_1, num_context_range, is_train=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = torch.tensor(np.array(features_df).astype(np.float32), dtype=torch.float32)\n",
        "        self.masks = torch.tensor(mask_df.values, dtype=torch.bool)\n",
        "        self.labels = torch.tensor(labels_series.values, dtype=torch.float32)\n",
        "        self.x_all_0 = coord_tensor_0\n",
        "        self.x_all_1 = coord_tensor_1\n",
        "\n",
        "        self.img_path_map = {os.path.basename(f): os.path.join(d, f) for d in IMG_DIRS if os.path.isdir(d) for f in os.listdir(d) if f.lower().endswith(('.png', '.jpg', '.jpeg'))}\n",
        "        self.samples = [{'df_idx': df_idx, 'img_filename': os.path.basename(fn)} for df_idx, img_paths_str in enumerate(image_paths_series.values) for fn in (ast.literal_eval(img_paths_str) if isinstance(img_paths_str, str) else []) if os.path.basename(fn) in self.img_path_map]\n",
        "\n",
        "        self.num_context_range = num_context_range\n",
        "        self.transform = transforms.Compose([transforms.Resize((IMAGE_RESOLUTION, IMAGE_RESOLUTION)), transforms.ToTensor(), transforms.Normalize([0.5] * 3, [0.5] * 3)])\n",
        "        print(f\"Dataset initialized with {len(self.samples)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_info = self.samples[idx]\n",
        "        df_idx, img_filename = sample_info['df_idx'], sample_info['img_filename']\n",
        "        y_sensors, sensor_mask, label = self.features[df_idx], self.masks[df_idx], self.labels[df_idx]\n",
        "\n",
        "        try:\n",
        "            full_img_path = self.img_path_map[img_filename]\n",
        "            img_tensor = self.transform(Image.open(full_img_path).convert(\"RGB\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping corrupted or missing file: {img_filename}. Error: {e}\")\n",
        "            return None\n",
        "\n",
        "        coord_tensor_to_use = self.x_all_1 if img_filename.endswith(\"_1.png\") else self.x_all_0\n",
        "\n",
        "        valid_indices = torch.where(sensor_mask)[0]\n",
        "        if len(valid_indices) < self.num_context_range[0]: return None\n",
        "\n",
        "        num_context_total = min(np.random.randint(*self.num_context_range), len(valid_indices))\n",
        "        shuffled_indices = valid_indices[torch.randperm(len(valid_indices))]\n",
        "        context_indices = shuffled_indices[:num_context_total]\n",
        "        target_indices = valid_indices # TargetÏùÄ Î™®Îì† Ïú†Ìö®Ìïú ÏÑºÏÑúÎ•º ÏÇ¨Ïö©\n",
        "\n",
        "        # --- [ÏàòÏ†ïÎêú Î°úÏßÅ] Ïª®ÌÖçÏä§Ìä∏ Ìè¨Ïù∏Ìä∏Î•º img_contextÏôÄ num_contextÎ°ú Î∂ÑÎ¶¨ ---\n",
        "        # Ïã§Ï†ú Ï¢åÌëúÍ∞íÏù¥ ÏûàÎäî (0,0Ïù¥ ÏïÑÎãå) ÏÑºÏÑúÎ•º 'img_context'Î°ú, Í∑∏Î†áÏßÄ ÏïäÏùÄ ÏÑºÏÑúÎ•º 'num_context'Î°ú Î∂ÑÎ•òÌï©ÎãàÎã§.\n",
        "        context_x_all = coord_tensor_to_use[context_indices]\n",
        "        context_y_all = y_sensors[context_indices].unsqueeze(-1)\n",
        "\n",
        "        # Ï¢åÌëúÍ∞Ä (0,0)Ïù¥ ÏïÑÎãå Í≤ΩÏö∞ TrueÏù∏ ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±\n",
        "        is_img_context_mask = torch.any(context_x_all != 0, dim=1)\n",
        "\n",
        "        img_context_x = context_x_all[is_img_context_mask]\n",
        "        img_context_y = context_y_all[is_img_context_mask]\n",
        "\n",
        "        num_context_x = context_x_all[~is_img_context_mask]\n",
        "        num_context_y = context_y_all[~is_img_context_mask]\n",
        "        # --- [Î°úÏßÅ ÏàòÏ†ï ÎÅù] ---\n",
        "\n",
        "        target_x, target_y = coord_tensor_to_use[target_indices], y_sensors[target_indices].unsqueeze(-1)\n",
        "        sensor_summary = y_sensors[sensor_mask].mean().unsqueeze(0) if len(valid_indices) > 0 else torch.tensor([0.0])\n",
        "\n",
        "        return {'num_context_x': num_context_x, 'num_context_y': num_context_y,\n",
        "                'img_context_x': img_context_x, 'img_context_y': img_context_y,\n",
        "                'target_x': target_x, 'target_y': target_y, 'image': img_tensor,\n",
        "                'label': label, 'sensor_summary': sensor_summary,\n",
        "                'target_mask': sensor_mask[target_indices]}\n",
        "\n",
        "# --- [ÏàòÏ†ï] Îç∞Ïù¥ÌÑ∞ Î°úÎçî ---\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None];\n",
        "    if not batch: return None\n",
        "    collated = {}\n",
        "    # num_context, img_context, targetÏùÑ Î™®Îëê Ï≤òÎ¶¨\n",
        "    for part in ['num_context', 'img_context', 'target']:\n",
        "        for dim in ['x', 'y']:\n",
        "            key = f'{part}_{dim}'; tensors = [item[key] for item in batch]\n",
        "            collated[key] = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    collated['image'] = torch.stack([item['image'] for item in batch])\n",
        "    collated['label'] = torch.stack([item['label'] for item in batch])\n",
        "    collated['sensor_summary'] = torch.stack([item['sensor_summary'] for item in batch])\n",
        "    collated['target_mask'] = torch.nn.utils.rnn.pad_sequence([item['target_mask'] for item in batch], batch_first=True, padding_value=False)\n",
        "    return collated\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "def preprocess_dataframe(df: pd.DataFrame):\n",
        "    print(\"--- [ÏµúÏ¢Ö] Ï†ÑÏ≤òÎ¶¨: Ï≤≠ÏÜå + Î≥¥Í∞Ñ(ffill/bfill) + Ï∞®Î∂Ñ + StandardScaler ---\")\n",
        "\n",
        "    if 'time' in df.columns:\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        df = df.sort_values('time').reset_index(drop=True)\n",
        "\n",
        "    ignore_cols = ['time', 'Images', 'label', 'file_num']\n",
        "    sensor_keys = [col for col in df.columns if col not in ignore_cols]\n",
        "\n",
        "    # 1. Float Î≥ÄÌôò & Î¨ºÎ¶¨Ï†Å Ïù¥ÏÉÅÏπò Ï†úÍ±∞\n",
        "    for col in sensor_keys:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
        "\n",
        "        # 10000 ÎÑòÎäî Í∞í Ï†úÍ±∞\n",
        "        mask_too_big = df[col] > 10000\n",
        "        if mask_too_big.sum() > 0: df.loc[mask_too_big, col] = np.nan\n",
        "\n",
        "        # ÎØ∏ÏÑ∏ Í∞í 0 Ï≤òÎ¶¨\n",
        "        mask_too_small = df[col].abs() < 1e-4\n",
        "        if mask_too_small.sum() > 0: df.loc[mask_too_small, col] = 0.0\n",
        "\n",
        "    # 2. [Î≥¥Í∞Ñ] Ï∞®Î∂Ñ Ï†ÑÏóê Îß§ÎÅÑÎüΩÍ≤å Î©îÍæ∏Í∏∞ (ÌïôÏäµ/Í≤ÄÏ¶ù ÎèôÏùº!)\n",
        "    # ÏïûÎí§ Í∞íÏúºÎ°ú Ï±ÑÏö∞Í≥†(ffill, bfill), Ï†ï ÏóÜÏúºÎ©¥ 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
        "    df[sensor_keys] = df[sensor_keys].ffill().bfill().fillna(0)\n",
        "\n",
        "    # 3. [Ï∞®Î∂Ñ] ÏÜçÎèÑ(Î≥ÄÌôîÎüâ) Í≥ÑÏÇ∞\n",
        "    # Ï∞®Î∂Ñ ÌõÑ Ï≤´ ÌñâÏùÄ NaNÏù¥ ÎêòÎØÄÎ°ú 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
        "    df[sensor_keys] = df[sensor_keys].diff().fillna(0)\n",
        "\n",
        "    # 4. Î©àÏ∂ò ÏÑºÏÑú ÏÇ≠Ï†ú & Ïä§ÏºÄÏùºÎßÅ Ï§ÄÎπÑ\n",
        "    normal_df = df[df['label'] == 0]\n",
        "    target_df = normal_df if not normal_df.empty else df\n",
        "\n",
        "    valid_sensor_keys = []\n",
        "    drop_cols = []\n",
        "\n",
        "    for col in sensor_keys:\n",
        "        values = target_df[col].dropna()\n",
        "        if len(values) == 0:\n",
        "            drop_cols.append(col)\n",
        "            continue\n",
        "\n",
        "        std = np.std(values)\n",
        "        if std == 0:\n",
        "            drop_cols.append(col)\n",
        "        else:\n",
        "            valid_sensor_keys.append(col)\n",
        "\n",
        "    print(f\"  ‚ùå ÏÇ≠Ï†ú(ÏôÑÏ†Ñ Í≥†Ï†ï): {len(drop_cols)}Í∞ú\")\n",
        "    print(f\"  ‚úÖ Ï†ÅÏö©(StandardScaler): {len(valid_sensor_keys)}Í∞ú\")\n",
        "\n",
        "    # 5. StandardScaler Ï†ÅÏö©\n",
        "    scaler = StandardScaler()\n",
        "    if valid_sensor_keys:\n",
        "        scaler.fit(target_df[valid_sensor_keys])\n",
        "        df[valid_sensor_keys] = scaler.transform(df[valid_sensor_keys])\n",
        "\n",
        "    # 6. Ï†ÄÏû•\n",
        "    mask_df = (df[valid_sensor_keys] != 0)\n",
        "\n",
        "    coord_tensor_0 = create_sensor_coord_tensor(valid_sensor_keys, coord_type=0)\n",
        "    coord_tensor_1 = create_sensor_coord_tensor(valid_sensor_keys, coord_type=1)\n",
        "\n",
        "    scaler_info = {\n",
        "        'scaler': scaler,\n",
        "        'valid_sensor_keys': valid_sensor_keys\n",
        "    }\n",
        "    joblib.dump(scaler_info, SCALER_SAVE_PATH)\n",
        "\n",
        "    return df, mask_df, scaler_info, valid_sensor_keys, coord_tensor_0, coord_tensor_1\n",
        "\n",
        "\n",
        "def process_validation_data(df: pd.DataFrame, scaler_info, sensor_keys_unused=None):\n",
        "\n",
        "    if isinstance(scaler_info, dict):\n",
        "        scaler = scaler_info['scaler']\n",
        "        final_sensor_keys = scaler_info['valid_sensor_keys']\n",
        "    else:\n",
        "        raise ValueError(\"Scaler Info Error\")\n",
        "\n",
        "    if 'time' in df.columns:\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        df = df.sort_values('time').reset_index(drop=True)\n",
        "\n",
        "    # 1. Ïª¨Îüº Ï≤¥ÌÅ¨ (ÎàÑÎùΩ Ïãú ÏóêÎü¨)\n",
        "    missing_cols = [col for col in final_sensor_keys if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"üö® Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Ïª¨Îüº ÎàÑÎùΩ: {missing_cols}\")\n",
        "\n",
        "    df_val = df[final_sensor_keys].copy()\n",
        "\n",
        "    # 2. Float Î≥ÄÌôò & Ï≤≠ÏÜå\n",
        "    for col in final_sensor_keys:\n",
        "        df_val[col] = pd.to_numeric(df_val[col], errors='coerce').astype(float)\n",
        "\n",
        "        mask_too_big = df_val[col] > 10000\n",
        "        if mask_too_big.sum() > 0: df_val.loc[mask_too_big, col] = np.nan\n",
        "\n",
        "        mask_too_small = df_val[col].abs() < 1e-4\n",
        "        if mask_too_small.sum() > 0: df_val.loc[mask_too_small, col] = 0.0\n",
        "\n",
        "    # 3. [Î≥¥Í∞Ñ] ÌïôÏäµ ÎïåÏôÄ 100% ÎèôÏùºÌïòÍ≤å Ï†ÅÏö©\n",
        "    # ÏïûÎí§ Í∞íÏúºÎ°ú Ï±ÑÏö∞Í≥†(ffill, bfill), Ï†ï ÏóÜÏúºÎ©¥ 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
        "    df_val[final_sensor_keys] = df_val[final_sensor_keys].ffill().bfill().fillna(0)\n",
        "\n",
        "    # 4. [Ï∞®Î∂Ñ] ÌïôÏäµ ÎïåÏôÄ 100% ÎèôÏùºÌïòÍ≤å Ï†ÅÏö©\n",
        "    df_val[final_sensor_keys] = df_val[final_sensor_keys].diff().fillna(0)\n",
        "\n",
        "    # 5. Ïä§ÏºÄÏùºÎßÅ Ï†ÅÏö©\n",
        "    if final_sensor_keys:\n",
        "        df_val[final_sensor_keys] = scaler.transform(df_val[final_sensor_keys])\n",
        "\n",
        "    mask_df = (df_val[final_sensor_keys] != 0)\n",
        "\n",
        "    return df_val, mask_df"
      ],
      "metadata": {
        "id": "vdF6k5uCcojH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÏµúÏ¢Ö ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï"
      ],
      "metadata": {
        "id": "-Cspga0Zcmbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "LATENT_DIM = 64\n",
        "HIDDEN_DIM = 128\n",
        "BETA = 5.0"
      ],
      "metadata": {
        "id": "K9mUk7MTcr0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌõàÎ†® Ìï®Ïàò ÏÑ§Ï†ï"
      ],
      "metadata": {
        "id": "pOVMqm9Pct5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ÌïôÏäµ/ÌèâÍ∞Ä Î£®ÌîÑ ---\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_epoch(model, dataloader, optimizer, device, clip_value, anp_loss_weight, cvae_loss_weight, is_train=True):\n",
        "    model.train() if is_train else model.eval()\n",
        "    total_loss, total_nll, total_cvae_loss = 0, 0, 0\n",
        "    pbar = tqdm(dataloader, desc=\"Training\" if is_train else \"Evaluating\", leave=False)\n",
        "\n",
        "    last_batch_attention = None\n",
        "\n",
        "    for batch in pbar:\n",
        "        if batch is None: continue\n",
        "        for key in batch: batch[key] = batch[key].to(device)\n",
        "        if is_train: optimizer.zero_grad()\n",
        "\n",
        "        # ====================================================\n",
        "        # [Dynamic Context Splitting] - ÏõêÏÉÅÎ≥µÍµ¨ (Random Context)\n",
        "        # ====================================================\n",
        "        full_context_x = torch.cat((batch['num_context_x'], batch['img_context_x']), dim=1)\n",
        "        full_context_y = torch.cat((batch['num_context_y'], batch['img_context_y']), dim=1)\n",
        "\n",
        "        num_points = full_context_x.shape[1]\n",
        "\n",
        "        # [ÏàòÏ†ï] Îã§Ïãú Random Context Î°úÏßÅÏúºÎ°ú Î≥µÍ∑Ä\n",
        "        # ÌïôÏäµ Ïãú: ÎûúÎç§ÌïòÍ≤å ÏùºÎ∂ÄÎßå Î≥¥Ïó¨Ï§å (Í¥ÄÍ≥Ñ ÌïôÏäµ Ïú†ÎèÑ)\n",
        "        # ÌèâÍ∞Ä Ïãú: Ïó¨Í∏∞ÏÑúÎäî ÏùºÎã® FullÏùÑ Ïì∞ÏßÄÎßå, extract_featuresÏóêÏÑú Ï†úÏñ¥ Í∞ÄÎä•\n",
        "        if is_train:\n",
        "            # ÏµúÏÜå 3Í∞ú ~ ÏµúÎåÄ Ï†ÑÏ≤¥ ÏÇ¨Ïù¥ÏóêÏÑú ÎûúÎç§\n",
        "            num_context = np.random.randint(3, num_points + 1)\n",
        "            indices = torch.randperm(num_points, device=device)\n",
        "            context_idx = indices[:num_context]\n",
        "\n",
        "            context_x = full_context_x[:, context_idx, :]\n",
        "            context_y = full_context_y[:, context_idx, :]\n",
        "            target_x = full_context_x\n",
        "            target_y = full_context_y\n",
        "        else:\n",
        "            # Í∏∞Î≥∏ Í≤ÄÏ¶ù(Validation) Î£®ÌîÑÏóêÏÑúÎäî Full Context ÏÇ¨Ïö© (ÌïôÏäµ ÏßÑÌñâÏÉÅÌô© Ï≤¥ÌÅ¨Ïö©)\n",
        "            # *Ïã§Ï†ú Ïù¥ÏÉÅÌÉêÏßÄ ÏÑ±Îä• ÌèâÍ∞ÄÎäî extract_featuresÏóêÏÑú ÎßàÏä§ÌÇπ Ï†ÅÏö©*\n",
        "            context_x = full_context_x\n",
        "            context_y = full_context_y\n",
        "            target_x = full_context_x\n",
        "            target_y = full_context_y\n",
        "\n",
        "        # ====================================================\n",
        "        # [Î™®Îç∏ Forward]\n",
        "        # ====================================================\n",
        "        condition = torch.cat([batch['label'].unsqueeze(1), batch['sensor_summary']], dim=1)\n",
        "\n",
        "        y_dist, anp_loss_dict, cvae_loss_dict, latent_attn, image_cross_attn, det_attn_weights = model(\n",
        "            batch['image'], condition, context_x, context_y,\n",
        "            target_x, target_y=target_y\n",
        "        )\n",
        "\n",
        "        # ÏÜêÏã§ Í≥ÑÏÇ∞\n",
        "        loss = anp_loss_weight * anp_loss_dict['loss'] + cvae_loss_weight * cvae_loss_dict['loss']\n",
        "        nll = anp_loss_dict.get('loss_p', 0.0)\n",
        "        cvae_loss = cvae_loss_dict.get('loss', 0.0)\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item(); total_nll += nll.item(); total_cvae_loss += cvae_loss.item()\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", nll=f\"{nll.item():.4f}\", cvae_loss=f\"{cvae_loss.item():.4f}\")\n",
        "\n",
        "        if not is_train and image_cross_attn is not None:\n",
        "            last_batch_attention = image_cross_attn.cpu().detach()\n",
        "\n",
        "    avg_loss = total_loss / len(pbar) if len(pbar) > 0 else 0\n",
        "    avg_nll = total_nll / len(pbar) if len(pbar) > 0 else 0\n",
        "    avg_cvae = total_cvae_loss / len(pbar) if len(pbar) > 0 else 0\n",
        "\n",
        "    return avg_loss, avg_nll, avg_cvae, last_batch_attention\n",
        "\n",
        "# --- Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò ---\n",
        "def main_train():\n",
        "    args = get_args()\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Hyperparameters: Clip={args.clip}, ANP Loss Weight={args.anp_loss_weight}, CVAE Loss Weight={args.cvae_loss_weight}\")\n",
        "\n",
        "    try:\n",
        "        train_raw_df = pd.read_excel(TRAIN_DATA_PATH)\n",
        "        val_raw_df = pd.read_excel(VAL_DATA_PATH)\n",
        "    except FileNotFoundError as e: print(f\"Error: Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÏóÜÏùå - {e}\"); return\n",
        "\n",
        "    train_df, train_mask, scaler, sensor_keys, coord_0, coord_1 = preprocess_dataframe(train_raw_df)\n",
        "    # `val_df` will contain only sensor data. `val_raw_df` is needed for 'Images' and 'label'.\n",
        "    val_df_processed, val_mask = process_validation_data(val_raw_df, scaler, sensor_keys)\n",
        "\n",
        "    # 1. ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ Í∞ïÏ†ú Î≥ÄÌôò\n",
        "    train_features_np = train_df[sensor_keys].to_numpy(dtype=np.float32)\n",
        "    # 2. Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Í∞ïÏ†ú Î≥ÄÌôò\n",
        "    val_features_np = val_df_processed[sensor_keys].to_numpy(dtype=np.float32)\n",
        "\n",
        "    train_dataset = CombinedANPDataset(train_features_np, train_mask, train_raw_df['Images'], train_raw_df['label'],\n",
        "                                       coord_0, coord_1, num_context_range=(10, 50))\n",
        "    val_dataset = CombinedANPDataset(val_features_np, val_mask, val_raw_df['Images'], val_raw_df['label'],\n",
        "                                     coord_0, coord_1, num_context_range=(10, 50), is_train=False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "    model = AttentiveCVAEANP(cvae_latent_dim=LATENT_DIM, condition_dim=2,\n",
        "                             x_dim=X_DIM, anp_latent_dim=LATENT_DIM,\n",
        "                             hidden_dim=HIDDEN_DIM, beta = BETA,\n",
        "                             img_size=IMAGE_RESOLUTION,\n",
        "                             min_std = 0.01, dropout = 0.2).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-3)\n",
        "    print(f\"\\nModel initialized on {DEVICE} with {sum(p.numel() for p in model.parameters()):,} parameters.\")\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_limit = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"\\n--- Î™®Îç∏ ÌïôÏäµ ÏãúÏûë ---\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "        train_loss, train_nll, train_cvae_loss, _ = run_epoch(model, train_loader, optimizer, DEVICE, args.clip,\n",
        "                                                              args.anp_loss_weight, args.cvae_loss_weight, is_train=True)\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_nll, val_cvae_loss, last_attention = run_epoch(model, val_loader, None,\n",
        "                                                                         DEVICE, args.clip, args.anp_loss_weight, args.cvae_loss_weight, is_train=False)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Train -> Total: {train_loss:.4f} | NLL: {train_nll:.4f} | CVAE: {train_cvae_loss:.4f}\")\n",
        "        print(f\"  Val   -> Total: {val_loss:.4f} | NLL: {val_nll:.4f} | CVAE: {val_cvae_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0 # Reset patience\n",
        "            torch.save(model.state_dict(), BEST_MODEL_SAVE_PATH)\n",
        "            print(f\"Validation loss improved. Model saved to {BEST_MODEL_SAVE_PATH}\")\n",
        "\n",
        "            if last_attention is not None:\n",
        "                torch.save(last_attention, BEST_ATTENTION_SAVE_PATH)\n",
        "                print(f\"Attention weights for best model saved to {BEST_ATTENTION_SAVE_PATH}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience_limit}\")\n",
        "            if patience_counter >= patience_limit:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "    print(\"\\n--- Training Finished ---\")"
      ],
      "metadata": {
        "id": "GPRx6_FUcwZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌõàÎ†®"
      ],
      "metadata": {
        "id": "nk1w0ptoczHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Save the original sys.argv\n",
        "_original_sys_argv = sys.argv\n",
        "\n",
        "# Temporarily set sys.argv to only the script name, ignoring Colab's internal arguments\n",
        "sys.argv = [sys.argv[0]]\n",
        "\n",
        "try:\n",
        "    if __name__ == \"__main__\":\n",
        "        main_train()\n",
        "finally:\n",
        "    # Restore sys.argv to its original state after main_train completes or an error occurs\n",
        "    sys.argv = _original_sys_argv\n"
      ],
      "metadata": {
        "id": "VDKZyFEdc0JM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "c2c31a89-93b9-4bb2-eb99-fd3c32770e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Hyperparameters: Clip=1.0, ANP Loss Weight=1.0, CVAE Loss Weight=1.0\n",
            "--- [ÏµúÏ¢Ö] Ï†ÑÏ≤òÎ¶¨: Ï≤≠ÏÜå + Î≥¥Í∞Ñ(ffill/bfill) + Ï∞®Î∂Ñ + StandardScaler ---\n",
            "  ‚ùå ÏÇ≠Ï†ú(ÏôÑÏ†Ñ Í≥†Ï†ï): 4Í∞ú\n",
            "  ‚úÖ Ï†ÅÏö©(StandardScaler): 47Í∞ú\n",
            "Dataset initialized with 38002 samples.\n",
            "Dataset initialized with 6000 samples.\n",
            "\n",
            "Model initialized on cuda with 11,005,413 parameters.\n",
            "\n",
            "--- Î™®Îç∏ ÌïôÏäµ ÏãúÏûë ---\n",
            "\n",
            "--- Epoch 1/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-587046308.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmain_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Restore sys.argv to its original state after main_train completes or an error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3400026399.py\u001b[0m in \u001b[0;36mmain_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         train_loss, train_nll, train_cvae_loss, _ = run_epoch(model, train_loader, optimizer, DEVICE, args.clip,\n\u001b[0m\u001b[1;32m    106\u001b[0m                                                               args.anp_loss_weight, args.cvae_loss_weight, is_train=True)\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3400026399.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, dataloader, optimizer, device, clip_value, anp_loss_weight, cvae_loss_weight, is_train)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlast_batch_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3701972655.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mfull_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_path_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_filename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Skipping corrupted or missing file: {img_filename}. Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CVAE-ANP ÌÖåÏä§Ìä∏"
      ],
      "metadata": {
        "id": "yo9LBoClKrCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Î∂ÑÌè¨ ÌôïÏù∏"
      ],
      "metadata": {
        "id": "pQ3p798SO3DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Í∏∞Ï°¥ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ïú†ÏßÄ)\n",
        "model = AttentiveCVAEANP(\n",
        "    cvae_latent_dim=LATENT_DIM,\n",
        "    condition_dim=2,\n",
        "    x_dim=X_DIM,\n",
        "    anp_latent_dim=LATENT_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    beta=BETA,\n",
        "    img_size=IMAGE_RESOLUTION,\n",
        "    min_std=0.01,\n",
        "    dropout=0.2\n",
        ").to(DEVICE)\n",
        "\n",
        "# 2. Í∞ÄÏ§ëÏπò Î°úÎìú\n",
        "if os.path.exists(BEST_MODEL_SAVE_PATH):\n",
        "    state_dict = torch.load(BEST_MODEL_SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(f\"‚úÖ Î™®Îç∏ Í∞ÄÏ§ëÏπò Î°úÎìú ÏÑ±Í≥µ: {BEST_MODEL_SAVE_PATH}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Í∞ÄÏ§ëÏπò ÌååÏùº ÏóÜÏùå: {BEST_MODEL_SAVE_PATH}\")\n",
        "\n",
        "try:\n",
        "    train_raw_df = pd.read_excel(TRAIN_DATA_PATH)\n",
        "    val_raw_df = pd.read_excel(VAL_DATA_PATH)\n",
        "except FileNotFoundError as e:\n",
        "  print(f\"Error: Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÏóÜÏùå - {e}\")\n",
        "\n",
        "train_df, train_mask, scaler, sensor_keys, coord_0, coord_1 = preprocess_dataframe(train_raw_df)\n",
        "# `val_df` will contain only sensor data. `val_raw_df` is needed for 'Images' and 'label'.\n",
        "val_df_processed, val_mask = process_validation_data(val_raw_df, scaler, sensor_keys)\n",
        "\n",
        "# 1. ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ Í∞ïÏ†ú Î≥ÄÌôò\n",
        "train_features_np = train_df[sensor_keys].to_numpy(dtype=np.float32)\n",
        "# 2. Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ Í∞ïÏ†ú Î≥ÄÌôò\n",
        "val_features_np = val_df_processed[sensor_keys].to_numpy(dtype=np.float32)\n",
        "\n",
        "train_dataset = CombinedANPDataset(train_features_np, train_mask, train_raw_df['Images'], train_raw_df['label'],\n",
        "                                   coord_0, coord_1, num_context_range=(10, 50))\n",
        "val_dataset = CombinedANPDataset(val_features_np, val_mask, val_raw_df['Images'], val_raw_df['label'],\n",
        "                                 coord_0, coord_1, num_context_range=(10, 50), is_train=False)\n",
        "\n",
        "# 3. [ÌïµÏã¨] Î∂ÑÏÑùÏö© Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ± (Batch Size = 1)\n",
        "# Í∑∏ÎûòÏïº ÏÉòÌîå ÌïòÎÇòÌïòÎÇòÏùò Ï†êÏàòÎ•º Î≥º Ïàò ÏûàÏäµÎãàÎã§.\n",
        "train_loader_analysis = DataLoader(\n",
        "    train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=0\n",
        ")\n",
        "val_loader_analysis = DataLoader(\n",
        "    val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=0\n",
        ")\n",
        "\n",
        "def analyze_distributions_final(model, dataloader, device, title=\"Data Analysis\"):\n",
        "    model.eval()\n",
        "\n",
        "    # Í≤∞Í≥º Ï†ÄÏû• Î¶¨Ïä§Ìä∏\n",
        "    results = {\n",
        "        'NLL': [],\n",
        "        'MSE': [],\n",
        "        'AttnDev': [],\n",
        "        'Label': [] # Ïã§Ï†ú Ï†ïÎãµ ÎùºÎ≤® (ÎÇòÏ§ëÏóê ÎπÑÍµêÏö©)\n",
        "    }\n",
        "\n",
        "    print(f\"[{title}] Ï†ïÎ∞Ä Î∂ÑÌè¨ Î∂ÑÏÑù Ï§ë (Batch=1)...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=title):\n",
        "            if batch is None: continue\n",
        "\n",
        "            # Îç∞Ïù¥ÌÑ∞Î•º ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô\n",
        "            for key in batch:\n",
        "                if isinstance(batch[key], torch.Tensor):\n",
        "                    batch[key] = batch[key].to(device)\n",
        "\n",
        "            # Context/Target ÏÑ§Ï†ï (Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©)\n",
        "            full_x = torch.cat((batch['num_context_x'], batch['img_context_x']), dim=1)\n",
        "            full_y = torch.cat((batch['num_context_y'], batch['img_context_y']), dim=1)\n",
        "\n",
        "            # Ï°∞Í±¥ (Condition)\n",
        "            condition = torch.cat([batch['label'].unsqueeze(1), batch['sensor_summary']], dim=1)\n",
        "\n",
        "            # Î™®Îç∏ Ï∂îÎ°†\n",
        "            y_dist, anp_loss_dict, cvae_loss_dict, _, image_cross_attn, _ = model(\n",
        "                batch['image'], condition, full_x, full_y, full_x, target_y=full_y\n",
        "            )\n",
        "\n",
        "            # 1. NLL (Anomaly Score)\n",
        "            # Î∞∞ÏπòÍ∞Ä 1Ïù¥ÎØÄÎ°ú item()ÏùÑ Ïì∞Î©¥ Ìï¥Îãπ ÏÉòÌîåÏùò Ï†êÏàòÍ∞Ä Îê®\n",
        "            nll = anp_loss_dict.get('loss_p', torch.tensor(0.0)).item()\n",
        "\n",
        "            # 2. MSE (Reconstruction Error)\n",
        "            mse = cvae_loss_dict.get('Reconstruction_Loss', torch.tensor(0.0)).item()\n",
        "\n",
        "            # 3. AttnDev (Attention Î∂àÌôïÏã§ÏÑ±)\n",
        "            if image_cross_attn is not None:\n",
        "                attn_dev = image_cross_attn.std().item()\n",
        "            else:\n",
        "                attn_dev = 0.0\n",
        "\n",
        "            # ÎùºÎ≤® (0: Ï†ïÏÉÅ, 1: Î∂àÎüâ)\n",
        "            label = batch['label'].item()\n",
        "\n",
        "            results['NLL'].append(nll)\n",
        "            results['MSE'].append(mse)\n",
        "            results['AttnDev'].append(attn_dev)\n",
        "            results['Label'].append(label)\n",
        "\n",
        "    # DataFrame Î≥ÄÌôò\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "\n",
        "    # --- ÏãúÍ∞ÅÌôî (GMM & Histogram) ---\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    fig.suptitle(f'Anomaly Score Distribution: {title}', fontsize=20, fontweight='bold')\n",
        "\n",
        "    cols = ['NLL', 'MSE', 'AttnDev']\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c'] # ÌååÎûë, Ï£ºÌô©, Ï¥àÎ°ù\n",
        "\n",
        "    for i, col in enumerate(cols):\n",
        "        data = metrics_df[col].values.reshape(-1, 1)\n",
        "\n",
        "        # 1. ÌûàÏä§ÌÜ†Í∑∏Îû® Í∑∏Î¶¨Í∏∞\n",
        "        sns.histplot(data=metrics_df, x=col, kde=True, ax=axes[i], color=colors[i], bins=50)\n",
        "\n",
        "        # 2. GMMÏúºÎ°ú ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ (Ï†ïÏÉÅ vs Ïû†Ïû¨Ï†Å Ïù¥ÏÉÅÏπò Î∂ÑÎ¶¨ ÏãúÎèÑ)\n",
        "        try:\n",
        "            gmm = GaussianMixture(n_components=2, random_state=42)\n",
        "            gmm.fit(data)\n",
        "            means = gmm.means_.flatten()\n",
        "            weights = gmm.weights_.flatten()\n",
        "\n",
        "            # Ï†ïÎ†¨ (ÏûëÏùÄ ÌèâÍ∑†Ïù¥ Î≥¥ÌÜµ Ï†ïÏÉÅ, ÌÅ∞ ÌèâÍ∑†Ïù¥ Ïù¥ÏÉÅÏπò)\n",
        "            idx = np.argsort(means)\n",
        "            normal_mean = means[idx[0]]\n",
        "            abnormal_mean = means[idx[1]]\n",
        "\n",
        "            axes[i].set_title(f'{col} (Normal Œº={normal_mean:.2f}, Abnormal Œº={abnormal_mean:.2f})')\n",
        "\n",
        "            # Í≤ΩÍ≥ÑÏÑ†(Threshold) ÌëúÏãú - Îëê Î∂ÑÌè¨Ïùò Ï§ëÍ∞Ñ ÏßÄÏ†ê\n",
        "            threshold = (normal_mean + abnormal_mean) / 2\n",
        "            axes[i].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold:.2f})')\n",
        "            axes[i].legend()\n",
        "\n",
        "        except Exception as e:\n",
        "            axes[i].set_title(f'{col} Distribution')\n",
        "            print(f\"GMM Error on {col}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "# --- Ïã§Ìñâ ---\n",
        "# 1. Train Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ (Í∏∞Ï§ÄÏ†ê Ïû°Í∏∞Ïö©)\n",
        "print(\"üìä Train Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂ÑÏÑù ÏãúÏûë...\")\n",
        "train_metrics_df = analyze_distributions_final(model, train_loader_analysis, DEVICE, title=\"Train Set (Normal Reference)\")\n",
        "\n",
        "# 2. Val Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ (Ïù¥ÏÉÅÏπò ÌÉêÏßÄÏö©)\n",
        "print(\"\\nüìä Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ Î∂ÑÏÑù ÏãúÏûë...\")\n",
        "val_metrics_df = analyze_distributions_final(model, val_loader_analysis, DEVICE, title=\"Validation Set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "K6tx_7gyDA0t",
        "outputId": "3222a847-8331-4d62-8621-acd9b18d8d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Î™®Îç∏ Í∞ÄÏ§ëÏπò Î°úÎìú ÏÑ±Í≥µ: /content/drive/MyDrive/ResultCA/best_cvae_anp_model.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2918115318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_raw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mval_raw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_DATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         data = io.parse(\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m   1614\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# doctest: +SKIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \"\"\"\n\u001b[0;32m-> 1616\u001b[0;31m         return self._reader.parse(\n\u001b[0m\u001b[1;32m   1617\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mfile_rows_needed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_rows_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"close\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0;31m# pyxlsb opens two TemporaryFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36mget_sheet_data\u001b[0;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mScalar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mlast_row_with_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0mconverted_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mconverted_row\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconverted_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_read_only.py\u001b[0m in \u001b[0;36m_cells_by_row\u001b[0;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[1;32m     83\u001b[0m                                      timedelta_formats=self.parent._timedelta_formats)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_row\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/worksheet/_reader.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add a finaliser to close the source when this becomes possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mtag_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36miterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                 \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_and_return_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0;34m\"\"\"Feed encoded data to parser.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raiseerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m../Modules/pyexpat.c\u001b[0m in \u001b[0;36mStartElement\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, tag, attr_list)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m         \u001b[0;31m# Handler for expat's StartElementHandler. Since ordered_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m         \u001b[0;31m# is set, the attributes are reported as a list of alternating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï"
      ],
      "metadata": {
        "id": "ySpm3kq9LaB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageDraw, ImageFont\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io\n",
        "import cv2 # ÌûàÌä∏Îßµ ÏÉùÏÑ±ÏùÑ ÏúÑÌï¥ Ï∂îÍ∞Ä\n",
        "\n",
        "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "VAL_DATA_PATH = \"/content/preproc_val_data_final.xlsx\"\n",
        "TEST_DATA_PATH = \"/content/preproc_test_data_final.xlsx\"\n",
        "SCALER_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/anp_scaler.joblib\"\n",
        "BEST_MODEL_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/best_cvae_anp_model.pth\"\n",
        "PREPROC_DATA_SAVE_PATH = \"/content/preproc_data.xlsx\" # (ÏòµÏÖò) ÏÑºÏÑú ÌÇ§ ÌôïÏù∏Ïö© ÌååÏùº Í≤ΩÎ°ú\n",
        "VIS_RESULT_DIR = \"/content/vis_result_final\" # ÏãúÍ∞ÅÌôî Í≤∞Í≥º Ï†ÄÏû• Ìè¥Îçî\n",
        "\n",
        "TRAIN_IMG_DIRS = [\n",
        "    \"/content/Final/Image/train/BATCH1000\", \"/content/Final/Image/train/BATCH2000\",\n",
        "    \"/content/Final/Image/train/BATCH3000\", \"/content/Final/Image/train/BATCH4000\",\n",
        "    \"/content/Final/Image/train/BATCH5000\", \"/content/Final/Image/train/BATCH6000\",\n",
        "    \"/content/Final/Image/train/BATCH7000\", \"/content/Final/Image/train/BATCH8000\",\n",
        "    \"/content/Final/Image/train/BATCH9000\", \"/content/Final/Image/train/BATCH10000\",\n",
        "    \"/content/Final/Image/train/BATCH11000\", \"/content/Final/Image/train/BATCH12000\",\n",
        "    \"/content/Final/Image/train/BATCH13000\", \"/content/Final/Image/train/BATCH14000\",\n",
        "    \"/content/Final/Image/train/BATCH15000\", \"/content/Final/Image/train/BATCH16000\",\n",
        "    \"/content/Final/Image/train/BATCH17000\", \"/content/Final/Image/train/BATCH18000\",\n",
        "    \"/content/Final/Image/train/BATCH19000\", \"/content/Final/Image/val/BATCH20000\",\n",
        "    \"/content/Final/Image/val/BATCH21000\", \"/content/Final/Image/val/BATCH22000\"\n",
        "]\n",
        "\n",
        "VAL_IMG_DIRS = [\n",
        "    \"/content/Final/Image/val/BATCH20000\",\n",
        "    \"/content/Final/Image/val/BATCH21000\",\n",
        "    \"/content/Final/Image/val/BATCH22000\"\n",
        "]\n",
        "\n",
        "TEST_IMG_DIRS = [\n",
        "    \"/content/Final/Image/test/BATCH24000\",\n",
        "    \"/content/Final/Image/test/BATCH71000\",\n",
        "    \"/content/Final/Image/test/BATCH72000\"\n",
        "]"
      ],
      "metadata": {
        "id": "2b_mlWT6LcnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌÖåÏä§Ìä∏ Ìï®Ïàò ÏÑ§Ï†ï"
      ],
      "metadata": {
        "id": "u89YqQ2yLeeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Î∂ÑÏÑùÏö© Ìï®Ïàò"
      ],
      "metadata": {
        "id": "0rQf0kQiLhqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# [Dataset & Collate Fn Ïû¨Ï†ïÏùò] 'file' ÌÇ§ ÎàÑÎùΩ Î∞©ÏßÄ\n",
        "# ====================================================\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, features, mask_df, image_paths_series, labels_series, coord_tensor_0, coord_tensor_1, image_dirs):\n",
        "        self.features = torch.tensor(np.array(features).astype(np.float32), dtype=torch.float32)\n",
        "        self.masks = torch.tensor(mask_df.values, dtype=torch.bool)\n",
        "        self.labels = torch.tensor(labels_series.values, dtype=torch.float32)\n",
        "        self.x_all_0 = coord_tensor_0\n",
        "        self.x_all_1 = coord_tensor_1\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_RESOLUTION, IMAGE_RESOLUTION)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "        ])\n",
        "\n",
        "        # Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ ÏµúÏ†ÅÌôî\n",
        "        self.img_map = {}\n",
        "        for d in image_dirs:\n",
        "            if os.path.exists(d):\n",
        "                for f in os.listdir(d):\n",
        "                    self.img_map[f] = os.path.join(d, f)\n",
        "\n",
        "        self.samples = []\n",
        "        for idx, path_str in enumerate(image_paths_series):\n",
        "            try:\n",
        "                filenames = ast.literal_eval(path_str) if isinstance(path_str, str) and '[' in str(path_str) else [path_str]\n",
        "                for f in filenames:\n",
        "                    base = os.path.basename(str(f))\n",
        "                    if base in self.img_map:\n",
        "                        self.samples.append({'idx': idx, 'file': base})\n",
        "            except: continue\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        s = self.samples[i]\n",
        "        feat, m, lbl = self.features[s['idx']], self.masks[s['idx']], self.labels[s['idx']]\n",
        "        img_filename = s['file']\n",
        "\n",
        "        try:\n",
        "            img_path = self.img_map[img_filename]\n",
        "            img = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
        "        except: return None\n",
        "\n",
        "        is_type_1 = 1 if img_filename.endswith(\"_1.png\") else 0\n",
        "        coord_tensor_to_use = self.x_all_1 if is_type_1 else self.x_all_0\n",
        "        v_idx = torch.where(m)[0]\n",
        "        if len(v_idx) == 0: return None\n",
        "\n",
        "        return {\n",
        "            'context_x': coord_tensor_to_use[v_idx], 'context_y': feat[v_idx].unsqueeze(-1),\n",
        "            'target_x': coord_tensor_to_use[v_idx], 'target_y': feat[v_idx].unsqueeze(-1),\n",
        "            'image': img, 'label': lbl,\n",
        "            'sensor_summary': feat[m].mean().unsqueeze(0),\n",
        "            'target_mask': m[v_idx], 'sensor_indices': v_idx,\n",
        "            'is_type_1': is_type_1,\n",
        "            'file': img_filename # ‚òÖ Ïó¨Í∏∞ÏÑú ÌååÏùºÎ™ÖÏùÑ Íº≠ Î¶¨ÌÑ¥Ìï¥Ïïº Ìï®\n",
        "        }\n",
        "\n",
        "def collate_fn_test(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if not batch: return None\n",
        "    collated = {}\n",
        "    for key in ['context_x', 'context_y', 'target_x', 'target_y', 'target_mask', 'sensor_indices']:\n",
        "        tensors = [item[key] for item in batch]\n",
        "        collated[key] = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    collated['image'] = torch.stack([item['image'] for item in batch])\n",
        "    collated['label'] = torch.stack([item['label'] for item in batch])\n",
        "    collated['sensor_summary'] = torch.stack([item['sensor_summary'] for item in batch])\n",
        "\n",
        "    # ‚òÖ Ïó¨Í∏∞ÏÑú 'file' ÌÇ§Î•º Î∞∞ÏπòÎ°ú Î¨∂Ïñ¥ÏÑú Î∞òÌôòÌï¥Ïïº KeyErrorÍ∞Ä Ïïà ÎÇ®\n",
        "    collated['file'] = [item['file'] for item in batch]\n",
        "\n",
        "    return collated\n",
        "\n",
        "def preprocess_test_dataframe(df, sensor_keys, scaler=None, is_train_ref=False):\n",
        "    # 1. Ïà´ÏûêÌòï Î≥ÄÌôò Î∞è Î¨ºÎ¶¨Ï†Å Ïù¥ÏÉÅÏπò(ÎÑàÎ¨¥ ÌÅ¨Í±∞ÎÇò ÏûëÏùÄ Í∞í) Ï†úÍ±∞\n",
        "    for col in sensor_keys:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
        "\n",
        "        # 10000 ÎÑòÎäî Í∞í Ï†úÍ±∞ (NaN Ï≤òÎ¶¨ ÌõÑ Î≥¥Í∞ÑÎê®)\n",
        "        mask_too_big = df[col] > 10000\n",
        "        if mask_too_big.sum() > 0: df.loc[mask_too_big, col] = np.nan\n",
        "\n",
        "        # ÎÑàÎ¨¥ ÏûëÏùÄ Í∞í(1e-4 ÎØ∏Îßå) 0ÏúºÎ°ú Ï≤òÎ¶¨\n",
        "        mask_too_small = df[col].abs() < 1e-4\n",
        "        if mask_too_small.sum() > 0: df.loc[mask_too_small, col] = 0.0\n",
        "\n",
        "    # 2. Î≥¥Í∞Ñ (Interpolation)\n",
        "    # ÏïûÎí§ Í∞íÏúºÎ°ú Ï±ÑÏö∞Í≥†(ffill, bfill), Ï†ï ÏóÜÏúºÎ©¥ 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
        "    df[sensor_keys] = df[sensor_keys].ffill().bfill().fillna(0)\n",
        "\n",
        "    # 3. Ï∞®Î∂Ñ (Differencing) - Î≥ÄÌôîÎüâ Í≥ÑÏÇ∞\n",
        "    df[sensor_keys] = df[sensor_keys].diff().fillna(0)\n",
        "\n",
        "    # 4. Ïä§ÏºÄÏùºÎßÅ (StandardScaler)\n",
        "    if is_train_ref:\n",
        "        # Reference(ÌïôÏäµ) Îç∞Ïù¥ÌÑ∞: ScalerÎ•º ÌïôÏäµ(fit)\n",
        "        if scaler is None: scaler = StandardScaler()\n",
        "        scaler.fit(df[sensor_keys])\n",
        "    else:\n",
        "        # Test Îç∞Ïù¥ÌÑ∞: Í∏∞Ï°¥ ScalerÎ•º Ï†ÅÏö©(transform)Îßå Ìï®\n",
        "        if scaler is None:\n",
        "            raise ValueError(\"Test mode requires a fitted scaler!\")\n",
        "        df[sensor_keys] = scaler.transform(df[sensor_keys])\n",
        "\n",
        "    return df, scaler\n",
        "\n",
        "def calculate_topk_patch_mse(images, recons, patch_size=16, k=5):\n",
        "    \"\"\"\n",
        "    Computes the mean MSE of the top-k patches with the highest error.\n",
        "    Images should be (B, C, H, W).\n",
        "    \"\"\"\n",
        "    diff = (images - recons) ** 2\n",
        "    mse_map = diff.mean(dim=1) # Average over channels\n",
        "\n",
        "    H, W = mse_map.shape[1], mse_map.shape[2]\n",
        "    if H % patch_size != 0 or W % patch_size != 0:\n",
        "        return mse_map.mean(dim=(1, 2))\n",
        "\n",
        "    # Unfold: (B, Grid_H, Grid_W, P, P)\n",
        "    patches = mse_map.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "\n",
        "    # Mean over patch area: (B, Grid_H, Grid_W)\n",
        "    patch_mse = patches.mean(dim=(3, 4))\n",
        "\n",
        "    # Flatten: (B, Num_Patches)\n",
        "    patch_mse_flat = patch_mse.reshape(patch_mse.size(0), -1)\n",
        "\n",
        "    # Top-K\n",
        "    k = min(k, patch_mse_flat.size(1))\n",
        "    topk_values, _ = torch.topk(patch_mse_flat, k=k, dim=1)\n",
        "\n",
        "    # Mean of Top-K\n",
        "    return topk_values.mean(dim=1)"
      ],
      "metadata": {
        "id": "234hMKn1Lriv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Í≤∞Í≥º Ï†ÄÏû•Ïö© Ìï®Ïàò"
      ],
      "metadata": {
        "id": "yU2VPL82Lxr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sensor_to_component(sensor_name):\n",
        "    for component in sorted(COMPONENT_ORDER, key=len, reverse=True):\n",
        "        if component in sensor_name: return component\n",
        "    return 'Conv' if 'Conv' in sensor_name else None\n",
        "\n",
        "def create_sensor_coord_tensor(sensor_keys, coord_type):\n",
        "    coords_map = COMPONENT_COORDS_1 if coord_type == 1 else COMPONENT_COORDS_0\n",
        "    coords = []\n",
        "    w, h = ORIGINAL_IMG_DIMS\n",
        "    for key in sensor_keys:\n",
        "        component_name = map_sensor_to_component(key)\n",
        "        if component_name and component_name in coords_map:\n",
        "            box = coords_map[component_name]\n",
        "            coords.append([(box['x1'] + box['x2']) / 2.0 / w, (box['y1'] + box['y2']) / 2.0 / h])\n",
        "        else:\n",
        "            coords.append([0.0, 0.0]) # Îß§Ìïë Î∂àÍ∞Ä Ïãú Í∏∞Î≥∏Í∞í\n",
        "    return torch.tensor(coords, dtype=torch.float32)\n",
        "\n",
        "def create_top_sensors_barplot(sensor_names, attention_scores):\n",
        "    \"\"\"Ï£ºÏñ¥ÏßÑ ÏÑºÏÑú Ïñ¥ÌÖêÏÖò Ï†êÏàòÎ°ú Top 5 ÎßâÎåÄÍ∑∏ÎûòÌîÑÎ•º ÏÉùÏÑ±ÌïòÍ≥† PIL Ïù¥ÎØ∏ÏßÄÎ°ú Î∞òÌôòÌï©ÎãàÎã§.\"\"\"\n",
        "    if not sensor_names or not attention_scores.any():\n",
        "        return None\n",
        "\n",
        "    # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± Î∞è Ï†ïÎ†¨\n",
        "    df = pd.DataFrame({'sensor': sensor_names, 'attention': attention_scores})\n",
        "    df_sorted = df.sort_values('attention', ascending=False).head(5)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.barplot(x='attention', y='sensor', data=df_sorted, ax=ax, palette='viridis', hue='sensor', legend=False)\n",
        "    ax.set_title('Top 5 Sensor Attention Importance', fontsize=16)\n",
        "    ax.set_xlabel('Average Attention on Image', fontsize=12)\n",
        "    ax.set_ylabel('Sensor', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png', dpi=100)\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "    plt.close(fig)\n",
        "    return img\n",
        "\n",
        "def create_reconstruction_heatmap(recon_x, original_image_path, save_path):\n",
        "    \"\"\"\n",
        "    ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ Ïû¨Íµ¨ÏÑ± Ïù¥ÎØ∏ÏßÄÏùò Ï∞®Ïù¥Î•º ÌûàÌä∏ÎßµÏúºÎ°ú ÏãúÍ∞ÅÌôîÌïòÍ≥† Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
        "    ÏÉâÏÉÅÏùÑ Î™ÖÌôïÌïòÍ≤å ÌïòÍ≥†, ColorbarÎ•º Ï∂îÍ∞ÄÌïòÏó¨ ÏóêÎü¨Ïùò Ï†ïÎèÑÎ•º ÌëúÏãúÌï©ÎãàÎã§.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import cv2\n",
        "    import os\n",
        "\n",
        "    # 1. ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
        "    if not os.path.exists(original_image_path):\n",
        "        print(f\"  - Warning: Original image not found at {original_image_path}. Skipping heatmap.\")\n",
        "        return\n",
        "\n",
        "    img_bgr = cv2.imread(original_image_path)\n",
        "    if img_bgr is None:\n",
        "        print(f\"  - Warning: Failed to load image at {original_image_path}. Skipping heatmap.\")\n",
        "        return\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 2. Ïû¨Íµ¨ÏÑ± Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ (Tensor -> Numpy, Denormalize)\n",
        "    # recon_xÎäî (C, H, W) ÌòïÌÉúÏùò TensorÎùºÍ≥† Í∞ÄÏ†ï\n",
        "    recon_img_np = recon_x.detach().cpu().numpy().transpose(1, 2, 0) # (H, W, C)\n",
        "\n",
        "    # Denormalize (ÌïôÏäµ Ïãú ÏÇ¨Ïö©Ìïú transformÏóê ÎßûÏ∂∞ Ï°∞Ï†ï ÌïÑÏöî)\n",
        "    # Ïó¨Í∏∞ÏÑúÎäî ÏùºÎ∞òÏ†ÅÏù∏ ImageNet ÌèâÍ∑†/ÌëúÏ§ÄÌé∏Ï∞®Î•º Í∞ÄÏ†ïÌïòÍ≥† Ïó≠Î≥ÄÌôòÌï©ÎãàÎã§.\n",
        "    # ÎßåÏïΩ Îã§Î•∏ Ï†ïÍ∑úÌôîÎ•º ÏÇ¨Ïö©ÌñàÎã§Î©¥ Í∑∏Ïóê ÎßûÍ≤å ÏàòÏ†ïÌï¥Ïïº Ìï©ÎãàÎã§.\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    recon_img_np = std * recon_img_np + mean\n",
        "    recon_img_np = np.clip(recon_img_np, 0, 1)\n",
        "    recon_img_rgb = (recon_img_np * 255).astype(np.uint8)\n",
        "\n",
        "    # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î¶¨ÏÇ¨Ïù¥Ï¶à (Ïû¨Íµ¨ÏÑ± Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Ïóê ÎßûÏ∂§)\n",
        "    img_rgb_resized = cv2.resize(img_rgb, (recon_img_rgb.shape[1], recon_img_rgb.shape[0]))\n",
        "\n",
        "    # 3. Ïû¨Íµ¨ÏÑ± Ïò§Ï∞® (MSE) Í≥ÑÏÇ∞ Î∞è ÌûàÌä∏Îßµ ÏÉùÏÑ±\n",
        "    # Ï±ÑÎÑêÎ≥Ñ Ï†úÍ≥± Ïò§Ï∞®Ïùò ÌèâÍ∑†ÏùÑ Íµ¨ÌïòÏó¨ 2D ÌûàÌä∏Îßµ ÏÉùÏÑ±\n",
        "    error_map = np.mean((img_rgb_resized.astype(np.float32) - recon_img_rgb.astype(np.float32))**2, axis=2)\n",
        "\n",
        "    # ÌûàÌä∏Îßµ Ï†ïÍ∑úÌôî (0~255, ÏãúÍ∞ÅÌôîÎ•º ÏúÑÌï¥)\n",
        "    # vmin, vmaxÎ•º ÏÑ§Ï†ïÌïòÏó¨ ÏÉâÏÉÅ ÎåÄÎπÑÎ•º Í∑πÎåÄÌôîÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
        "    # Ïó¨Í∏∞ÏÑúÎäî Ï†ÑÏ≤¥ Î≤îÏúÑÎ°ú Ï†ïÍ∑úÌôîÌï©ÎãàÎã§.\n",
        "    error_map_norm = cv2.normalize(error_map, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    # Ïª¨Îü¨Îßµ Ï†ÅÏö© (JET Colormap ÏÇ¨Ïö©ÏúºÎ°ú Î™ÖÌôïÌïú ÏÉâÏÉÅ ÌëúÌòÑ)\n",
        "    heatmap_color = cv2.applyColorMap(error_map_norm, cv2.COLORMAP_JET)\n",
        "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 4. ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏôÄ ÌûàÌä∏Îßµ Ìï©ÏÑ±\n",
        "    # alpha Í∞íÏùÑ Ï°∞Ï†àÌïòÏó¨ ÌûàÌä∏ÎßµÏùò Ìà¨Î™ÖÎèÑÎ•º Í≤∞Ï†ïÌï©ÎãàÎã§.\n",
        "    alpha = 0.5\n",
        "    overlay_img = cv2.addWeighted(img_rgb_resized, 1 - alpha, heatmap_color, alpha, 0)\n",
        "\n",
        "    # 5. ÏãúÍ∞ÅÌôî Î∞è Ï†ÄÏû• (Colorbar Ï∂îÍ∞Ä)\n",
        "    plt.figure(figsize=(8, 6)) # Í∑∏Î¶º ÌÅ¨Í∏∞ ÏÑ§Ï†ï\n",
        "\n",
        "    # Ìï©ÏÑ±Îêú Ïù¥ÎØ∏ÏßÄ ÌëúÏãú\n",
        "    plt.imshow(overlay_img)\n",
        "\n",
        "    # Colorbar Ï∂îÍ∞ÄÎ•º ÏúÑÌïú ScalarMappable ÏÉùÏÑ±\n",
        "    # Ïã§Ï†ú ÏóêÎü¨ Í∞íÏùò Î≤îÏúÑÎ•º ÌëúÏãúÌïòÍ∏∞ ÏúÑÌï¥ Ï†ïÍ∑úÌôî Ï†ÑÏùò error_mapÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    norm = plt.Normalize(vmin=error_map.min(), vmax=error_map.max())\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([]) # Îπà Î∞∞Ïó¥ ÏÑ§Ï†ï ÌïÑÏöî\n",
        "\n",
        "    # Colorbar Í∑∏Î¶¨Í∏∞\n",
        "    cbar = plt.colorbar(sm, ax=plt.gca(), fraction=0.046, pad=0.04)\n",
        "    cbar.set_label('Reconstruction Error (MSE)', rotation=270, labelpad=15)\n",
        "\n",
        "    plt.title(f\"Reconstruction Anomaly Heatmap\\n'{os.path.basename(original_image_path)}'\")\n",
        "    plt.axis('off') # Ï∂ï Ïà®ÍπÄ\n",
        "\n",
        "    # Ï†ÄÏû•\n",
        "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)\n",
        "    plt.close()\n",
        "    print(f\"  - Saved anomaly heatmap with colorbar to {save_path}\")\n",
        "\n",
        "def generate_anomaly_report(image_path, component_coords, high_error_sensors, sensor_to_component_map, output_path, case_type_str):\n",
        "    \"\"\"\n",
        "    Ïù¥ÏÉÅ ÌÉêÏßÄ Î¶¨Ìè¨Ìä∏Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§. ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóê Î∞îÏö¥Îî© Î∞ïÏä§Î•º Í∑∏Î¶¨Í≥†,\n",
        "    ÌïòÎã®Ïóê ÏÉÅÏÑ∏Ìïú Î∂ÑÏÑù ÎÇ¥Ïö©ÏùÑ Ìè¨Ìï®ÌïòÎäî ÌÜµÌï© Ïù¥ÎØ∏ÏßÄÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§„ÄÇ\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font_title = ImageFont.load_default()\n",
        "    font_text = ImageFont.load_default()\n",
        "\n",
        "    drawn_components = set()\n",
        "    if high_error_sensors:\n",
        "        for sensor_name in high_error_sensors.keys():\n",
        "            component = sensor_to_component_map.get(sensor_name)\n",
        "            if component and component in component_coords and component not in drawn_components:\n",
        "                coords = component_coords[component]\n",
        "                draw.rectangle([coords['x1'], coords['y1'], coords['x2'], coords['y2']], outline=\"red\", width=5)\n",
        "\n",
        "                text = f\"{component}\"\n",
        "                text_bbox = draw.textbbox((coords['x1'], coords['y1'] - 25), text, font=font_title)\n",
        "                draw.rectangle(text_bbox, fill=\"red\")\n",
        "                draw.text((coords['x1'], coords['y1'] - 25), text, fill=\"white\", font=font_title)\n",
        "                drawn_components.add(component)\n",
        "\n",
        "    report_lines = [\n",
        "        \"Anomaly Detection Report\",\n",
        "        f\"File: {os.path.basename(image_path)}\",\n",
        "        f\"Anomaly Type: {case_type_str}\",\n",
        "        \"=\"*60,\n",
        "        \"Anomaly Analysis Details:\"\n",
        "    ]\n",
        "\n",
        "    if high_error_sensors:\n",
        "        sorted_sensors = sorted(high_error_sensors.items(), key=lambda item: item[1]['error'], reverse=True)\n",
        "        for sensor_name, data in sorted_sensors:\n",
        "            component = sensor_to_component_map.get(sensor_name, 'N/A')\n",
        "            report_lines.append(f\"\\n- Sensor '{sensor_name}' (in Component: {component})\")\n",
        "            report_lines.append(f\"  - Actua Change (Delta)l: {data['actual_delta']:.4f}\")\n",
        "            report_lines.append(f\"  - Predicted Change (Delta): {data['predicted_delta']:.4f}\")\n",
        "            report_lines.append(f\"  - Uncertainty (StdDev): {data['predicted_std']:.4f}\")\n",
        "            report_lines.append(f\"  - Recon. Error: {data['error']:.4f}\")\n",
        "\n",
        "    report_text = \"\\n\".join(report_lines)\n",
        "\n",
        "    dummy_draw = ImageDraw.Draw(Image.new('RGB', (0, 0)))\n",
        "    text_height = dummy_draw.textbbox((0, 0), report_text, font=font_text)[3]\n",
        "    padding = 20\n",
        "\n",
        "    canvas_width = img.width\n",
        "    canvas_height = img.height + text_height + padding * 2\n",
        "    canvas = Image.new('RGB', (canvas_width, canvas_height), 'white')\n",
        "\n",
        "    canvas.paste(img, (0, 0))\n",
        "\n",
        "    current_h = img.height + padding\n",
        "    text_draw = ImageDraw.Draw(canvas)\n",
        "    text_draw.text((padding, current_h), report_text, font=font_text, fill=\"black\")\n",
        "\n",
        "    canvas.save(output_path)\n",
        "    print(f\"Saved anomaly report to {output_path}\")"
      ],
      "metadata": {
        "id": "Sv2v8K0aLzN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ÏãúÍ∞ÅÌôî Ìï®Ïàò"
      ],
      "metadata": {
        "id": "BPfPx3-ZL0kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# [Core Function 4] ÏãúÍ∞ÅÌôî (Î∞±Î∂ÑÏúÑÏàò ÏûÑÍ≥ÑÍ∞í Ìè¨Ìï®)\n",
        "# ====================================================\n",
        "def visualize_all_distributions(ref_results, test_results, thresholds):\n",
        "    print(\"\\n--- üìä [Visualization] Metrics & Thresholds ---\")\n",
        "\n",
        "    gt_labels = []\n",
        "    for fnum in test_results['file_num']:\n",
        "        if 23001 <= fnum <= 23500: gt_labels.append('Case 1')\n",
        "        elif 23501 <= fnum <= 24000: gt_labels.append('Case 2')\n",
        "        elif 71501 <= fnum <= 72000: gt_labels.append('Case 3')\n",
        "        else: gt_labels.append('Normal')\n",
        "    test_results['Class'] = gt_labels\n",
        "\n",
        "    metrics = ['nll', 'mse', 'attn']\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
        "\n",
        "    # Row 1: Ï†ïÏÉÅ Îç∞Ïù¥ÌÑ∞ (ÌïôÏäµÏÖã) Î∂ÑÌè¨ + ÏûÑÍ≥ÑÍ∞í\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[0, i]\n",
        "        data = ref_results[metric].dropna()\n",
        "        thresh = thresholds.get(metric, 0)\n",
        "\n",
        "        sns.histplot(data, bins=50, kde=True, ax=ax, color='royalblue', stat='density', alpha=0.4)\n",
        "        ax.axvline(thresh, color='red', linestyle='--', linewidth=2, label=f'Threshold')\n",
        "\n",
        "        # NLL, AttnÏùÄ Î°úÍ∑∏ Ïä§ÏºÄÏùºÏù¥ Î≥¥Í∏∞ Ï¢ãÏùÑ Ïàò ÏûàÏùå\n",
        "        if metric in ['nll', 'attn'] and data.min() > 0:\n",
        "            ax.set_xscale('log')\n",
        "\n",
        "        ax.set_title(f\"[Ref] {metric.upper()} Distribution\")\n",
        "        ax.legend()\n",
        "\n",
        "    # Row 2: ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ (ÌÅ¥ÎûòÏä§Î≥Ñ) Î∂ÑÌè¨ + ÏûÑÍ≥ÑÍ∞í\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[1, i]\n",
        "        thresh = thresholds.get(metric, 0)\n",
        "        plot_data = test_results.dropna(subset=[metric])\n",
        "\n",
        "        try:\n",
        "            sns.kdeplot(data=plot_data, x=metric, hue='Class', fill=True, alpha=0.2, linewidth=2,\n",
        "                        palette={'Normal': 'green', 'Case 1': 'red', 'Case 2': 'orange', 'Case 3': 'blue'},\n",
        "                        ax=ax, common_norm=False, warn_singular=False)\n",
        "        except:\n",
        "            sns.histplot(data=plot_data, x=metric, hue='Class', element=\"step\", stat=\"density\", common_norm=False, ax=ax)\n",
        "\n",
        "        ax.axvline(thresh, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
        "        if metric in ['nll', 'attn'] and plot_data[metric].min() > 0:\n",
        "            ax.set_xscale('log')\n",
        "\n",
        "        ax.set_title(f\"[Test] {metric.upper()} Distribution by Class\")\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QQ_m42yyL2cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ïû¨Íµ¨ÏÑ± ÌîºÏ≥ê Ï∂îÏ∂ú"
      ],
      "metadata": {
        "id": "UxBN4-5tL45y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# [Final Fixed] Cross-Modal Feature Extraction\n",
        "# ====================================================\n",
        "def extract_cross_modal_features(model, df, img_dirs, scaler, sensor_keys, coord_0, coord_1, device, desc=\"Cross-Modal\"):\n",
        "    features_np = df[sensor_keys].to_numpy(dtype=np.float32)\n",
        "    mask_df = (df[sensor_keys] != 0)\n",
        "\n",
        "    dataset = TestDataset(features_np, mask_df, df['Images'], df['label'],\n",
        "                          coord_0, coord_1, img_dirs)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        return pd.DataFrame(), np.array([])\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False,\n",
        "                                         collate_fn=collate_fn_test, num_workers=0)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    results = {\n",
        "        'i2s_mse': [],\n",
        "        'i2s_max_error': [],\n",
        "        's2i_mse': [],\n",
        "        'sigma': [],\n",
        "        'sensor_std': [],\n",
        "        'label': [], 'file_num': [], 'max_z_score': []\n",
        "    }\n",
        "\n",
        "    latent_dim = model.cvae.latent_dim\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=desc):\n",
        "            if batch is None: continue\n",
        "            for k in ['context_x', 'context_y', 'target_x', 'target_y', 'image', 'label', 'sensor_summary']:\n",
        "                if k in batch: batch[k] = batch[k].to(device)\n",
        "\n",
        "            # ---------------------------------------------------------\n",
        "            # (1) I2S: Image -> Sensor Prediction\n",
        "            # ---------------------------------------------------------\n",
        "            batch_size = batch['image'].shape[0]\n",
        "            dummy_condition = torch.zeros_like(torch.cat([batch['label'].unsqueeze(1), batch['sensor_summary']], dim=1))\n",
        "\n",
        "            # Dummy Point\n",
        "            empty_ctx_x = torch.zeros((batch_size, 1, batch['context_x'].shape[-1]), device=device)\n",
        "            empty_ctx_y = torch.zeros((batch_size, 1, batch['context_y'].shape[-1]), device=device)\n",
        "\n",
        "            ret = model(batch['image'], dummy_condition, empty_ctx_x, empty_ctx_y, batch['target_x'], target_y=batch['target_y'])\n",
        "            y_dist_pred = ret[0]\n",
        "\n",
        "            pred_y = y_dist_pred.loc\n",
        "            true_y = batch['target_y']\n",
        "\n",
        "            # 1. MSE (Mean)\n",
        "            i2s_error = torch.mean((pred_y - true_y) ** 2).item()\n",
        "\n",
        "            # 2. Max Error (ÏàòÏ†ïÎê®)\n",
        "            # .values Ï†úÍ±∞ -> torch.max()Îäî Ïä§ÏπºÎùº ÌÖêÏÑúÎ•º Î∞òÌôòÌïòÎØÄÎ°ú Î∞îÎ°ú .item() Ìò∏Ï∂ú\n",
        "            i2s_max = torch.max((pred_y - true_y) ** 2).item()\n",
        "\n",
        "            # 3. Sigma\n",
        "            sigma_val = y_dist_pred.scale.mean().item()\n",
        "\n",
        "            # 4. Sensor Std\n",
        "            sensor_std_val = torch.std(true_y).item()\n",
        "\n",
        "            # ---------------------------------------------------------\n",
        "            # (2) S2I: Sensor -> Image Generation\n",
        "            # ---------------------------------------------------------\n",
        "            real_condition = torch.cat([batch['label'].unsqueeze(1), batch['sensor_summary']], dim=1)\n",
        "            z_sample = torch.zeros((batch_size, latent_dim), device=device)\n",
        "            generated_img = model.cvae.decode(z_sample, real_condition)\n",
        "            s2i_error = torch.mean((generated_img - batch['image']) ** 2).item()\n",
        "\n",
        "            # ---------------------------------------------------------\n",
        "            # Ï†ÄÏû•\n",
        "            # ---------------------------------------------------------\n",
        "            results['i2s_mse'].append(i2s_error)\n",
        "            results['i2s_max_error'].append(i2s_max)\n",
        "            results['s2i_mse'].append(s2i_error)\n",
        "            results['sigma'].append(sigma_val)\n",
        "            results['sensor_std'].append(sensor_std_val)\n",
        "\n",
        "            results['label'].append(batch['label'].item())\n",
        "\n",
        "            fname = batch['file'][0]\n",
        "            try: fnum = int(fname.split('_')[0])\n",
        "            except: fnum = -1\n",
        "            results['file_num'].append(fnum)\n",
        "\n",
        "            sensor_vals = batch['target_y'].cpu().numpy().flatten()\n",
        "            max_z = np.max(np.abs(sensor_vals)) if len(sensor_vals) > 0 else 0.0\n",
        "            results['max_z_score'].append(max_z)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ====================================================\n",
        "# [Main] Ïã§Ìñâ Ìï®Ïàò (Phase 1: Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±Ïö©)\n",
        "# ====================================================\n",
        "def run_cross_modal_experiment():\n",
        "    print(\"üöÄ [Phase 1] Extracting Features for ALL Test Data...\")\n",
        "\n",
        "    # 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Î™®Îç∏ Ï§ÄÎπÑ\n",
        "    train_df = pd.read_excel(TRAIN_DATA_PATH)\n",
        "    val_df = pd.read_excel(VAL_DATA_PATH)\n",
        "    ref_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    sensor_keys = [c for c in ref_df.columns if c not in ['time', 'Images', 'label', 'file_num']]\n",
        "\n",
        "    clean_ref_df, scaler = preprocess_test_dataframe(ref_df.copy(), sensor_keys, is_train_ref=True)\n",
        "    coord_0 = create_sensor_coord_tensor(sensor_keys, 0)\n",
        "    coord_1 = create_sensor_coord_tensor(sensor_keys, 1)\n",
        "\n",
        "    model = AttentiveCVAEANP(cvae_latent_dim=LATENT_DIM, condition_dim=2, x_dim=X_DIM,\n",
        "                             hidden_dim=HIDDEN_DIM, anp_latent_dim=LATENT_DIM,\n",
        "                             beta=BETA, img_size=IMAGE_RESOLUTION, min_std=0.01, dropout=0.2).to(DEVICE)\n",
        "\n",
        "    if os.path.exists(BEST_MODEL_SAVE_PATH):\n",
        "        model.load_state_dict(torch.load(BEST_MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    else:\n",
        "        print(f\"Error: Model not found at {BEST_MODEL_SAVE_PATH}\")\n",
        "        return\n",
        "\n",
        "    # 2. Ï†ÑÏ≤¥ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ ÌäπÏßï Ï∂îÏ∂ú\n",
        "    print(\"\\n--- Extracting Cross-Modal Features (Transformation) ---\")\n",
        "    test_raw_df = pd.read_excel(TEST_DATA_PATH)\n",
        "    clean_test_df, _ = preprocess_test_dataframe(test_raw_df.copy(), sensor_keys, scaler=scaler)\n",
        "\n",
        "    test_results = extract_cross_modal_features(model, clean_test_df, TEST_IMG_DIRS, scaler, sensor_keys, coord_0, coord_1, DEVICE)\n",
        "\n",
        "    # 3. Ï†ïÎãµ ÎùºÎ≤®(GT) Ï∂îÍ∞Ä - [ÏàòÏ†ïÎê®: append ÎàÑÎùΩ Ìï¥Í≤∞]\n",
        "    gt_list = []\n",
        "    for fnum in test_results['file_num']:\n",
        "        if 23001 <= fnum <= 23500: gt = 1 # Mixed\n",
        "        elif 23501 <= fnum <= 24000: gt = 2 # Sensor Only\n",
        "        elif 71501 <= fnum <= 72000: gt = 3 # Visual Only\n",
        "        else: gt = 0\n",
        "        gt_list.append(gt) # <--- [Ï§ëÏöî] Îπ†Ï†∏ÏûàÎçò ÏΩîÎìú Ï∂îÍ∞ÄÎê®\n",
        "\n",
        "    test_results['gt'] = gt_list\n",
        "\n",
        "    # 4. Ï†ÄÏû•\n",
        "    save_path = \"cross_modal_results.csv\"\n",
        "    test_results.to_csv(save_path, index=False)\n",
        "    print(f\"\\n‚úÖ Feature extraction complete! Saved to '{save_path}'\")\n",
        "    print(\"üëâ Now run the XGBoost evaluation script.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_cross_modal_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV5w8vNiL57w",
        "outputId": "bdd43aa1-9a59-46fa-b1ee-04697773e947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ [Phase 1] Extracting Features for ALL Test Data...\n",
            "\n",
            "--- Extracting Cross-Modal Features (Transformation) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cross-Modal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [03:41<00:00, 27.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Feature extraction complete! Saved to 'cross_modal_results.csv'\n",
            "üëâ Now run the XGBoost evaluation script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost ÌõàÎ†® Î∞è Í≤∞Í≥º Ï∂úÎ†•"
      ],
      "metadata": {
        "id": "oZlvitLPWapH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "try:\n",
        "    df = pd.read_csv(\"cross_modal_results.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: 'cross_modal_results.csv' not found. Please run the extraction code first.\")\n",
        "    exit()\n",
        "\n",
        "# 2. ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ (8Í∞ú ÌîºÏ≤ò)\n",
        "# Î°úÍ∑∏ Î≥ÄÌôò Ïãú 0Ïù¥ ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú np.log1p ÏÇ¨Ïö©\n",
        "df['log_i2s'] = np.log1p(df['i2s_mse'])\n",
        "df['log_s2i'] = np.log1p(df['s2i_mse'])\n",
        "df['log_i2s_max'] = np.log1p(df['i2s_max_error']) # [New]\n",
        "df['diff_error'] = df['log_i2s'] - df['log_s2i']\n",
        "df['ratio_error'] = df['i2s_mse'] / (df['s2i_mse'] + 1e-6)\n",
        "\n",
        "features = [\n",
        "    'log_i2s', 'log_s2i', 'log_i2s_max',\n",
        "    'diff_error', 'ratio_error',\n",
        "    'max_z_score', 'sigma', 'sensor_std'\n",
        "]\n",
        "\n",
        "# Í≤∞Ï∏°Ïπò ÌôïÏù∏ Î∞è Ï≤òÎ¶¨ (ÏïàÏ†ÑÏùÑ ÏúÑÌï¥)\n",
        "X = df[features].fillna(0)\n",
        "y = df['gt']\n",
        "\n",
        "# 3. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (50:50)\n",
        "X_tune, X_eval, y_tune, y_eval = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"‚úÇÔ∏è Data Split: Tuning={len(X_tune)}, Evaluation={len(X_eval)}\")\n",
        "\n",
        "# 4. Grid Search ÏÑ§Ï†ï\n",
        "# Î≤îÏúÑÎ•º ÎÑìÍ≤å Ïû°ÏïÑÏÑú ÏµúÏ†ÅÏ†êÏùÑ ÌôïÏã§Ìûà Ï∞æÏäµÎãàÎã§.\n",
        "param_grid = {\n",
        "    'n_estimators': [200, 300, 500],      # ÎÇòÎ¨¥ Í∞úÏàò: ÎßéÏùÑÏàòÎ°ù Ï†ïÍµêÌï®\n",
        "    'max_depth': [4, 6, 8, 10],           # ÍπäÏù¥: Case 1/3 Íµ¨Î∂ÑÏùÑ ÏúÑÌï¥ ÍπäÍ≤å ÌååÎ¥Ñ\n",
        "    'learning_rate': [0.01, 0.05, 0.1],   # ÌïôÏäµÎ•†: ÏûëÏùÑÏàòÎ°ù ÍººÍººÌï®\n",
        "    'subsample': [0.8, 1.0],              # Îç∞Ïù¥ÌÑ∞ ÏÉòÌîåÎßÅ: Í≥ºÏ†ÅÌï© Î∞©ÏßÄ\n",
        "    'colsample_bytree': [0.8, 1.0],       # ÌîºÏ≤ò ÏÉòÌîåÎßÅ: ÌäπÏ†ï ÌîºÏ≤ò Ìé∏Ìñ• Î∞©ÏßÄ\n",
        "    'gamma': [0, 0.1, 0.2]                # Î¶¨ÌîÑ ÎÖ∏Îìú Ï∂îÍ∞ÄÎ•º ÏúÑÌïú ÏµúÏÜå ÏÜêÏã§ Í∞êÏÜåÍ∞í\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=4,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    n_jobs=-1 # Î≥ëÎ†¨ Ï≤òÎ¶¨\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Starting Grid Search... (This may take a few minutes)\")\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_tune, y_tune)\n",
        "\n",
        "print(f\"\\nüèÜ Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"   Best CV Score (Tuning Set): {grid_search.best_score_:.2%}\")\n",
        "\n",
        "# 5. ÏµúÏ¢Ö ÌèâÍ∞Ä (Evaluation Set)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_eval)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       [Grid Search Optimized] XGBoost Report\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_eval, y_pred, target_names=['Normal', 'Case 1', 'Case 2', 'Case 3'], digits=4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_eval, y_pred))\n",
        "print(f\"\\nüî• Final Accuracy: {accuracy_score(y_eval, y_pred):.2%}\")\n",
        "\n",
        "# 6. ÌîºÏ≤ò Ï§ëÏöîÎèÑ (ÏµúÏ¢Ö)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sorted_idx = best_model.feature_importances_.argsort()\n",
        "plt.barh([features[i] for i in sorted_idx], best_model.feature_importances_[sorted_idx], color='purple')\n",
        "plt.title(\"Optimized Feature Importance\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G5HFXkCoPgpk",
        "outputId": "b717b314-0298-4584-82c0-48c2deaa0b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÇÔ∏è Data Split: Tuning=3000, Evaluation=3000\n",
            "\n",
            "üîç Starting Grid Search... (This may take a few minutes)\n",
            "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
            "\n",
            "üèÜ Best Parameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300, 'subsample': 0.8}\n",
            "   Best CV Score (Tuning Set): 86.47%\n",
            "\n",
            "============================================================\n",
            "       [Grid Search Optimized] XGBoost Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal     0.9532    0.9507    0.9519      1500\n",
            "      Case 1     0.7738    0.7800    0.7769       500\n",
            "      Case 2     0.8781    0.9220    0.8995       500\n",
            "      Case 3     0.7684    0.7300    0.7487       500\n",
            "\n",
            "    accuracy                         0.8807      3000\n",
            "   macro avg     0.8434    0.8457    0.8443      3000\n",
            "weighted avg     0.8800    0.8807    0.8802      3000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1426   14   45   15]\n",
            " [  19  390    0   91]\n",
            " [  35    0  461    4]\n",
            " [  16  100   19  365]]\n",
            "\n",
            "üî• Final Accuracy: 88.07%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ïù¥ÏÉÅ ÏßÑÎã® Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"
      ],
      "metadata": {
        "id": "OG-vJWHliqbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import json\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from tqdm import tqdm\n",
        "\n",
        "# [ÏÑ§Ï†ï] Í≤ΩÎ°ú\n",
        "REPORT_DIR = \"/content/drive/MyDrive/anomaly_reports\"\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "TEST_DATA_PATH = \"/content/preproc_test_data_final.xlsx\"\n",
        "SCALER_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/anp_scaler.joblib\"\n",
        "BEST_MODEL_SAVE_PATH = \"/content/drive/MyDrive/ResultCA/best_cvae_anp_model.pth\"\n",
        "\n",
        "# XGBoost Í¥ÄÎ†® Í≤ΩÎ°ú\n",
        "XGB_MODEL_PATH = \"/content/drive/MyDrive/VAE-ANP/final_models/xgboost_classifier_88acc.pkl\"\n",
        "FEATURE_CONFIG_PATH = \"/content/drive/MyDrive/VAE-ANP/final_models/feature_config.json\"\n",
        "FEATURE_DATA_PATH = \"cross_modal_results.csv\" # Feature Extraction Îã®Í≥ÑÏóêÏÑú ÏÉùÏÑ±Îêú ÌååÏùº\n",
        "\n",
        "# CVAE Î™®Îç∏ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "LATENT_DIM = 64; HIDDEN_DIM = 128; X_DIM = 2; BETA = 5.0; IMAGE_RESOLUTION = 256\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"\\nüöÄ Setting up for Anomaly Reporting...\")\n",
        "\n",
        "# ====================================================\n",
        "# [ÏÑ§Ï†ï Î∞è ÏÉÅÏàò]\n",
        "# ====================================================\n",
        "label_map = {0: 'Normal', 1: 'Case 1 (Mixed)', 2: 'Case 2 (Sensor)', 3: 'Case 3 (Visual)'}\n",
        "COMPONENT_ORDER = ['MHS', 'R01', 'R02', 'R03', 'R04', 'Conv1', 'Conv2', 'Conv3', 'Conv4']\n",
        "\n",
        "# 1. ÏÑºÏÑú-Ïª¥Ìè¨ÎÑåÌä∏ Îß§Ìïë Ìï®Ïàò Ï†ïÏùò\n",
        "def map_sensor_to_component(sensor_name):\n",
        "    for component in sorted(COMPONENT_ORDER, key=len, reverse=True):\n",
        "        if component in sensor_name: return component\n",
        "    return 'Conv' if 'Conv' in sensor_name else None\n",
        "\n",
        "COMPONENT_COORDS_0 = {\n",
        "    'MHS': {'h': 53, 'w': 183, 'x1': 232, 'x2': 415, 'y1': 182, 'y2': 235},\n",
        "    'R01': {'h': 160, 'w': 99, 'x1': 385, 'x2': 484, 'y1': 102, 'y2': 262},\n",
        "    'R02': {'h': 148, 'w': 140, 'x1': 273, 'x2': 413, 'y1': 143, 'y2': 291},\n",
        "    'R03': {'h': 187, 'w': 173, 'x1': 372, 'x2': 545, 'y1': 161, 'y2': 348},\n",
        "    'R04': {'h': 139, 'w': 131, 'x1': 173, 'x2': 304, 'y1': 145, 'y2': 284},\n",
        "    'Conv1': {'h': 158, 'w': 334, 'x1': 414, 'x2': 748, 'y1': 362, 'y2': 520},\n",
        "    'Conv2': {'h': 139, 'w': 346, 'x1': 380, 'x2': 726, 'y1': 210, 'y2': 349},\n",
        "    'Conv3': {'h': 43, 'w': 225, 'x1': 131, 'x2': 356, 'y1': 228, 'y2': 271},\n",
        "    'Conv4': {'h': 294, 'w': 380, 'x1': 91, 'x2': 471, 'y1': 268, 'y2': 562}\n",
        "}\n",
        "COMPONENT_COORDS_1 = {\n",
        "    'MHS': {'h': 251, 'w': 397, 'x1': 363, 'x2': 760, 'y1': 278, 'y2': 529},\n",
        "    'R01': {'h': 439, 'w': 227, 'x1': 199, 'x2': 426, 'y1': 124, 'y2': 563},\n",
        "    'R02': {'h': 187, 'w': 128, 'x1': 379, 'x2': 507, 'y1': 108, 'y2': 295},\n",
        "    'R03': {'h': 148, 'w': 120, 'x1': 287, 'x2': 407, 'y1': 106, 'y2': 254},\n",
        "    'R04': {'h': 264, 'w': 268, 'x1': 589, 'x2': 857, 'y1': 164, 'y2': 428},\n",
        "    'Conv1': {'h': 39, 'w': 183, 'x1': 179, 'x2': 362, 'y1': 178, 'y2': 217},\n",
        "    'Conv2': {'h': 182, 'w': 301, 'x1': 168, 'x2': 469, 'y1': 208, 'y2': 390},\n",
        "    'Conv3': {'h': 81, 'w': 215, 'x1': 457, 'x2': 672, 'y1': 264, 'y2': 345},\n",
        "    'Conv4': {'h': 106, 'w': 338, 'x1': 354, 'x2': 692, 'y1': 173, 'y2': 279}\n",
        "}\n",
        "\n",
        "# ====================================================\n",
        "# 1. CVAE-ANP ÌôòÍ≤Ω ÏÑ§Ï†ï (Scaler & Model)\n",
        "# ====================================================\n",
        "if os.path.exists(SCALER_SAVE_PATH):\n",
        "    scaler_info = joblib.load(SCALER_SAVE_PATH)\n",
        "    scaler = scaler_info['scaler']\n",
        "    sensor_keys = scaler_info['valid_sensor_keys']\n",
        "    print(f\"‚úÖ Loaded Scaler & {len(sensor_keys)} Sensor Keys.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"‚ùå Scaler file not found: {SCALER_SAVE_PATH}\")\n",
        "\n",
        "# SENSOR_MAP ÏÉùÏÑ± (scaler Î°úÎìú ÌõÑ)\n",
        "SENSOR_MAP = {key: map_sensor_to_component(key) for key in sensor_keys}\n",
        "\n",
        "coord_0 = create_sensor_coord_tensor(sensor_keys, 0)\n",
        "coord_1 = create_sensor_coord_tensor(sensor_keys, 1)\n",
        "\n",
        "model = AttentiveCVAEANP(cvae_latent_dim=LATENT_DIM, condition_dim=2, x_dim=X_DIM,\n",
        "                         hidden_dim=HIDDEN_DIM, anp_latent_dim=LATENT_DIM,\n",
        "                         beta=BETA, img_size=IMAGE_RESOLUTION, min_std=0.01, dropout=0.2).to(DEVICE)\n",
        "\n",
        "if os.path.exists(BEST_MODEL_SAVE_PATH):\n",
        "    model.load_state_dict(torch.load(BEST_MODEL_SAVE_PATH, map_location=DEVICE))\n",
        "    print(f\"‚úÖ Loaded CVAE-ANP Weights\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"‚ùå Model weights not found: {BEST_MODEL_SAVE_PATH}\")\n",
        "\n",
        "# ====================================================\n",
        "# 2. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ\n",
        "# ====================================================\n",
        "print(\"\\n‚è≥ Loading TEST Data...\")\n",
        "try:\n",
        "    test_raw_df = pd.read_excel(TEST_DATA_PATH)\n",
        "    # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú ÏÑ§Ï†ï (ÌÖåÏä§Ìä∏Ïö© Ìè¥Îçî)\n",
        "    global IMG_DIRS\n",
        "    IMG_DIRS = [\n",
        "        \"/content/Final/Image/test/BATCH24000\",\n",
        "        \"/content/Final/Image/test/BATCH71000\",\n",
        "        \"/content/Final/Image/test/BATCH72000\"\n",
        "    ]\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Test Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÏóÜÏùå: {TEST_DATA_PATH}\")\n",
        "    exit()\n",
        "\n",
        "test_df_processed, test_mask = process_validation_data(test_raw_df, scaler_info, sensor_keys)\n",
        "test_features_np = test_df_processed[sensor_keys].to_numpy(dtype=np.float32)\n",
        "\n",
        "test_dataset = CombinedANPDataset(\n",
        "    test_features_np,\n",
        "    test_mask,\n",
        "    test_raw_df['Images'],\n",
        "    test_raw_df['label'],\n",
        "    coord_0,\n",
        "    coord_1,\n",
        "    num_context_range=(10, 50),\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# ====================================================\n",
        "# 3. XGBoost Ï∂îÎ°† Î∞è Í≤∞Í≥º Ï†ÄÏû•\n",
        "# ====================================================\n",
        "print(\"\\nüîÆ Running XGBoost Classification...\")\n",
        "\n",
        "# 3-1. Feature ÌååÏùº ÌôïÏù∏\n",
        "if not os.path.exists(FEATURE_DATA_PATH):\n",
        "    # Ïù¥Î¶ÑÏù¥ Îã§Î•º Ïàò ÏûàÏúºÎØÄÎ°ú Ï≤¥ÌÅ¨\n",
        "    if os.path.exists(\"cross_modal_results_final.csv\"):\n",
        "        FEATURE_DATA_PATH = \"cross_modal_results_final.csv\"\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"‚ùå Feature CSV not found. Please run feature extraction first.\")\n",
        "\n",
        "df_features = pd.read_csv(FEATURE_DATA_PATH)\n",
        "\n",
        "# 3-2. XGBoost Î™®Îç∏ Î°úÎìú\n",
        "if not os.path.exists(XGB_MODEL_PATH) or not os.path.exists(FEATURE_CONFIG_PATH):\n",
        "    raise FileNotFoundError(\"‚ùå XGBoost model or config file missing.\")\n",
        "\n",
        "xgb_model = joblib.load(XGB_MODEL_PATH)\n",
        "with open(FEATURE_CONFIG_PATH, 'r') as f:\n",
        "    feat_config = json.load(f)\n",
        "\n",
        "features_used = feat_config['features']\n",
        "print(f\"‚úÖ Loaded XGBoost. Using features: {features_used}\")\n",
        "\n",
        "# 3-3. Feature Engineering (ÌïôÏäµ ÎïåÏôÄ ÎèôÏùºÌïòÍ≤å)\n",
        "df_features['log_i2s'] = np.log1p(df_features['i2s_mse'])\n",
        "df_features['log_s2i'] = np.log1p(df_features['s2i_mse'])\n",
        "df_features['log_i2s_max'] = np.log1p(df_features['i2s_max_error'])\n",
        "df_features['diff_error'] = df_features['log_i2s'] - df_features['log_s2i']\n",
        "df_features['ratio_error'] = df_features['i2s_mse'] / (df_features['s2i_mse'] + 1e-6)\n",
        "\n",
        "# 3-4. ÏòàÏ∏° ÏàòÌñâ\n",
        "X_test = df_features[features_used].fillna(0)\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# 3-5. Í≤∞Í≥º Ï†ÄÏû•\n",
        "df_features['pred'] = y_pred\n",
        "df_features.to_csv(\"xgb_classification_results.csv\", index=True)\n",
        "print(f\"üíæ Classification results saved. Found {len(df_features[df_features['pred'] != 0])} anomalies.\")\n",
        "\n",
        "# ====================================================\n",
        "# 4. Ï∂îÎ°† Î∞è Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Ìï®Ïàò (ÏóêÎü¨ Î∞©ÏßÄ Í∞ïÌôî)\n",
        "# ====================================================\n",
        "def get_cvae_anp_output(model, dataset, data_index, device):\n",
        "    model.eval()\n",
        "    sample = dataset[data_index]\n",
        "    if sample is None: return None\n",
        "\n",
        "    batch = collate_fn([sample])\n",
        "    for key in batch:\n",
        "        if isinstance(batch[key], torch.Tensor):\n",
        "            batch[key] = batch[key].to(device)\n",
        "\n",
        "    # ÏûÖÎ†• Ï§ÄÎπÑ\n",
        "    full_x = torch.cat((batch['num_context_x'], batch['img_context_x']), dim=1)\n",
        "    full_y = torch.cat((batch['num_context_y'], batch['img_context_y']), dim=1)\n",
        "    condition = torch.cat([batch['label'].unsqueeze(1), batch['sensor_summary']], dim=1)\n",
        "\n",
        "    # [Cross-Modal Inference]\n",
        "    # 1. S2I Reconstruction (Sensor -> Image)\n",
        "    # 2. I2S Prediction (Image -> Sensor)\n",
        "    with torch.no_grad():\n",
        "        # --- I2S: Image Context Only ---\n",
        "        batch_size = batch['image'].shape[0]\n",
        "        empty_ctx_x = torch.zeros((batch_size, 1, full_x.shape[-1]), device=device)\n",
        "        empty_ctx_y = torch.zeros((batch_size, 1, 1), device=device)\n",
        "        dummy_cond = torch.zeros_like(condition)\n",
        "\n",
        "        returns = model(\n",
        "            batch['image'], dummy_cond, empty_ctx_x, empty_ctx_y, full_x, target_y=None\n",
        "        )\n",
        "        y_dist = returns[0]\n",
        "        image_cross_attn = returns[4]\n",
        "\n",
        "        # --- S2I: Sensor Condition Only ---\n",
        "        z_zero = torch.zeros((batch_size, model.cvae.latent_dim), device=device)\n",
        "        recon_img_tensor = model.cvae.decode(z_zero, condition)\n",
        "\n",
        "        # ÏÑºÏÑú Ïò§Ï∞® Í≥ÑÏÇ∞ (I2S)\n",
        "        pred_sensor_mean = y_dist.loc\n",
        "        pred_sensor_std = y_dist.scale\n",
        "\n",
        "        # (Batch, Seq, 1) -> (Seq,)\n",
        "        sensor_errors = torch.abs(full_y - pred_sensor_mean).squeeze(-1).cpu().numpy()[0]\n",
        "        sensor_uncertainty = pred_sensor_std.squeeze(-1).cpu().numpy()[0]\n",
        "        actual_deltas = full_y.squeeze(-1).cpu().numpy()[0]\n",
        "        predicted_deltas = pred_sensor_mean.squeeze(-1).cpu().numpy()[0]\n",
        "\n",
        "    # [ÏàòÏ†ï] Ïñ¥ÌÖêÏÖò Ï∞®Ïõê ÏóêÎü¨ Î∞©ÏßÄ\n",
        "    if image_cross_attn.dim() == 4:\n",
        "        att_score = image_cross_attn.mean(dim=[0, 1, 3]).cpu().numpy()\n",
        "    elif image_cross_attn.dim() == 3:\n",
        "        att_score = image_cross_attn.mean(dim=[0, 2]).cpu().numpy()\n",
        "    else:\n",
        "        att_score = np.full((full_x.shape[1],), image_cross_attn.mean().item())\n",
        "\n",
        "    # Ï¢åÌëú Îß§Ïπ≠ Î∞è ÌååÏùº Í≤ΩÎ°ú\n",
        "    img_filename = dataset.samples[data_index]['img_filename']\n",
        "    is_type_1 = img_filename.endswith('_1.png')\n",
        "    ref_coords = dataset.x_all_1 if is_type_1 else dataset.x_all_0\n",
        "\n",
        "    # full_x[0] (Seq, 2)\n",
        "    num_scored = len(att_score)\n",
        "    used_coords = full_x[0, :num_scored, :].to(device)\n",
        "\n",
        "    dists = torch.cdist(used_coords, ref_coords.to(device))\n",
        "    closest_indices = torch.argmin(dists, dim=1).cpu().numpy()\n",
        "    active_names = [sensor_keys[i] for i in closest_indices]\n",
        "\n",
        "    full_path = dataset.img_path_map.get(img_filename, img_filename)\n",
        "\n",
        "    # ÏÑºÏÑú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ± (Î¶¨Ìè¨Ìä∏Ïö©)\n",
        "    sensor_detail = {}\n",
        "    for i, name in enumerate(active_names):\n",
        "        if i < len(sensor_errors):\n",
        "            sensor_detail[name] = {\n",
        "                'error': float(sensor_errors[i]),\n",
        "                'actual_delta': float(actual_deltas[i]),\n",
        "                'predicted_delta': float(predicted_deltas[i]),\n",
        "                'predicted_std': float(sensor_uncertainty[i])\n",
        "            }\n",
        "\n",
        "    return {\n",
        "        'recon_img': recon_img_tensor[0].cpu() if recon_img_tensor.dim() == 4 else recon_img_tensor.cpu(),\n",
        "        'att_scores': att_score,\n",
        "        'img_path': full_path,\n",
        "        'active_sensors': active_names,\n",
        "        'sensor_info': sensor_detail,\n",
        "        'is_type_1': is_type_1\n",
        "    }\n",
        "\n",
        "# ====================================================\n",
        "# 5. Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ± Î©îÏù∏ Î£®ÌîÑ\n",
        "# ====================================================\n",
        "print(\"\\nüöÄ Starting Report Generation Loop...\")\n",
        "\n",
        "MAX_REPORTS = 20\n",
        "count = 0\n",
        "\n",
        "# anomaliesÎäî feature csvÏóêÏÑú Î°úÎìúÎêú Í≤É\n",
        "anomalies = df_features[df_features['pred'] != 0]\n",
        "\n",
        "for idx, row in anomalies.iterrows():\n",
        "    try:\n",
        "        # 1. XGBoost ÌåêÎã® Í≤∞Í≥º\n",
        "        pred_label = int(row['pred'])\n",
        "\n",
        "        # 2. CVAE-ANP Ï†ïÎ∞Ä Ï∂îÎ°†\n",
        "        res = get_cvae_anp_output(model, test_dataset, idx, DEVICE)\n",
        "        if res is None: continue\n",
        "\n",
        "        case_name = label_map[pred_label]\n",
        "        save_base = os.path.join(REPORT_DIR, f\"Report_{idx}_{case_name.replace(' ', '_')}\")\n",
        "\n",
        "        # ÌÉÄÏûÖÏóê ÎßûÎäî Ï¢åÌëúÍ≥Ñ ÏÑ†ÌÉù\n",
        "        current_coords = COMPONENT_COORDS_1 if res['is_type_1'] else COMPONENT_COORDS_0\n",
        "\n",
        "        print(f\"[{count+1}] Generating: ID {idx} - {case_name}\")\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # CaseÎ≥Ñ ÏãúÍ∞ÅÌôî Î°úÏßÅ\n",
        "        # ---------------------------------------------------------\n",
        "\n",
        "        # Case 1, 2: ÏÑºÏÑú Ï§ëÏã¨ Î¶¨Ìè¨Ìä∏ (generate_anomaly_report Ìò∏Ï∂ú)\n",
        "        if pred_label in [1, 2]:\n",
        "            # ÎßâÎåÄÍ∑∏ÎûòÌîÑ\n",
        "            bar_img = create_top_sensors_barplot(res['active_sensors'], res['att_scores'])\n",
        "            if bar_img: bar_img.save(f\"{save_base}_Barplot.png\")\n",
        "\n",
        "            # ÏÉÅÏÑ∏ Î¶¨Ìè¨Ìä∏ Ïù¥ÎØ∏ÏßÄ (Î∞îÏö¥Îî© Î∞ïÏä§ Ìè¨Ìï®)\n",
        "            # ÏóêÎü¨Í∞Ä Í∞ÄÏû• ÌÅ∞ Top 5 ÏÑºÏÑú Ï∂îÏ∂ú\n",
        "            sorted_sensors = sorted(res['sensor_info'].items(), key=lambda x: x[1]['error'], reverse=True)[:5]\n",
        "            top_sensors_dict = dict(sorted_sensors)\n",
        "\n",
        "            generate_anomaly_report(\n",
        "                image_path=res['img_path'],\n",
        "                component_coords=current_coords,\n",
        "                high_error_sensors=top_sensors_dict,\n",
        "                sensor_to_component_map=SENSOR_MAP,\n",
        "                output_path=f\"{save_base}_Info.png\",\n",
        "                case_type_str=case_name\n",
        "            )\n",
        "\n",
        "        # Case 1, 3: Ïù¥ÎØ∏ÏßÄ Ï§ëÏã¨ ÌûàÌä∏Îßµ\n",
        "        if pred_label in [1, 3]:\n",
        "            create_reconstruction_heatmap(res['recon_img'], res['img_path'], f\"{save_base}_Heatmap.png\")\n",
        "\n",
        "        count += 1\n",
        "        if count >= MAX_REPORTS: break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error at index {idx}: {e}\")\n",
        "        # import traceback; traceback.print_exc()\n",
        "\n",
        "print(f\"\\n‚úÖ Finished! {count} reports saved in {REPORT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiq2xkSs2Tuv",
        "outputId": "f4c73c71-605a-4c3b-c928-1f2c686732b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Setting up for Anomaly Reporting...\n",
            "‚úÖ Loaded Scaler & 47 Sensor Keys.\n",
            "‚úÖ Loaded CVAE-ANP Weights\n",
            "\n",
            "‚è≥ Loading TEST Data...\n",
            "Dataset initialized with 6000 samples.\n",
            "\n",
            "üîÆ Running XGBoost Classification...\n",
            "‚úÖ Loaded XGBoost. Using features: ['log_i2s', 'log_s2i', 'log_i2s_max', 'diff_error', 'ratio_error', 'max_z_score', 'sigma', 'sensor_std']\n",
            "üíæ Classification results saved. Found 3004 anomalies.\n",
            "\n",
            "üöÄ Starting Report Generation Loop...\n",
            "[1] Generating: ID 1 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_1_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_1_Case_1_(Mixed)_Heatmap.png\n",
            "[2] Generating: ID 2 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_2_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_2_Case_1_(Mixed)_Heatmap.png\n",
            "[3] Generating: ID 3 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_3_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_3_Case_1_(Mixed)_Heatmap.png\n",
            "[4] Generating: ID 4 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_4_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_4_Case_1_(Mixed)_Heatmap.png\n",
            "[5] Generating: ID 5 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_5_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_5_Case_1_(Mixed)_Heatmap.png\n",
            "[6] Generating: ID 6 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_6_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_6_Case_1_(Mixed)_Heatmap.png\n",
            "[7] Generating: ID 7 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_7_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_7_Case_1_(Mixed)_Heatmap.png\n",
            "[8] Generating: ID 8 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_8_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_8_Case_1_(Mixed)_Heatmap.png\n",
            "[9] Generating: ID 9 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_9_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_9_Case_1_(Mixed)_Heatmap.png\n",
            "[10] Generating: ID 10 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_10_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_10_Case_1_(Mixed)_Heatmap.png\n",
            "[11] Generating: ID 11 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_11_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_11_Case_1_(Mixed)_Heatmap.png\n",
            "[12] Generating: ID 12 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_12_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_12_Case_1_(Mixed)_Heatmap.png\n",
            "[13] Generating: ID 13 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_13_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_13_Case_1_(Mixed)_Heatmap.png\n",
            "[14] Generating: ID 14 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_14_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_14_Case_1_(Mixed)_Heatmap.png\n",
            "[15] Generating: ID 15 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_15_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_15_Case_1_(Mixed)_Heatmap.png\n",
            "[16] Generating: ID 16 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_16_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_16_Case_1_(Mixed)_Heatmap.png\n",
            "[17] Generating: ID 17 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_17_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_17_Case_1_(Mixed)_Heatmap.png\n",
            "[18] Generating: ID 19 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_19_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_19_Case_1_(Mixed)_Heatmap.png\n",
            "[19] Generating: ID 20 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_20_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_20_Case_1_(Mixed)_Heatmap.png\n",
            "[20] Generating: ID 23 - Case 1 (Mixed)\n",
            "Saved anomaly report to /content/drive/MyDrive/anomaly_reports/Report_23_Case_1_(Mixed)_Info.png\n",
            "  - Saved anomaly heatmap with colorbar to /content/drive/MyDrive/anomaly_reports/Report_23_Case_1_(Mixed)_Heatmap.png\n",
            "\n",
            "‚úÖ Finished! 20 reports saved in /content/drive/MyDrive/anomaly_reports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Ï†ÄÏû•Ìï† Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "SAVE_DIR = \"/content/drive/MyDrive/VAE-ANP/final_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 1. ÏµúÏ†ÅÏùò ÌååÎùºÎØ∏ÌÑ∞Î°ú ÌïôÏäµÎêú ÏµúÏ¢Ö Î™®Îç∏ (Grid SearchÏùò best_estimator_)\n",
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "# 2. Î™®Îç∏ Ï†ÄÏû• (XGBoost)\n",
        "joblib.dump(final_model, os.path.join(SAVE_DIR, \"xgboost_classifier_88acc.pkl\"))\n",
        "\n",
        "# 3. ÌîºÏ≤ò Î¶¨Ïä§Ìä∏ Ï†ÄÏû• (ÎÇòÏ§ëÏóê ÏàúÏÑú Ìó∑Í∞àÎ¶¨ÏßÄ ÏïäÍ≤å)\n",
        "# Î™®Îç∏ÏùÑ Ïì∏ Îïå Ïù¥ ÏàúÏÑúÎåÄÎ°ú Îç∞Ïù¥ÌÑ∞Î•º ÎÑ£Ïñ¥Ï§òÏïº Ìï©ÎãàÎã§.\n",
        "feature_config = {\n",
        "    \"features\": features,\n",
        "    \"description\": \"Final XGBoost model with Cross-Modal features\",\n",
        "    \"accuracy\": \"88.07%\"\n",
        "}\n",
        "\n",
        "with open(os.path.join(SAVE_DIR, \"feature_config.json\"), \"w\") as f:\n",
        "    json.dump(feature_config, f, indent=4)\n",
        "\n",
        "print(f\"\\n‚úÖ All files saved in '{SAVE_DIR}/' directory:\")\n",
        "print(f\"   1. xgboost_classifier_88acc.pkl (The Model)\")\n",
        "print(f\"   2. feature_config.json (Feature List)\")\n",
        "print(\"\\nüéâ Project Complete! Great Job!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP1IfdwPf-Ya",
        "outputId": "6d590e41-b8e9-4fc6-df63-b678a7368003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All files saved in '/content/drive/MyDrive/VAE-ANP/final_models/' directory:\n",
            "   1. xgboost_classifier_88acc.pkl (The Model)\n",
            "   2. feature_config.json (Feature List)\n",
            "\n",
            "üéâ Project Complete! Great Job!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE-LSTM ÌõàÎ†®"
      ],
      "metadata": {
        "id": "L4yaiII2cN5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "a40PxjivdQY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# project_root_path = '/content/drive/My Drive/my_project'\n",
        "project_root_path = '/content/drive/My Drive/VAE-LSTM/' # VAE_ANP Ìè¥Îçî ÏûêÏ≤¥Í∞Ä sys.pathÏóê Ï∂îÍ∞ÄÎêòÎèÑÎ°ù ÏàòÏ†ï\n",
        "\n",
        "# sys.pathÏóê Í≤ΩÎ°ú Ï∂îÍ∞Ä\n",
        "if project_root_path not in sys.path:\n",
        "    sys.path.append(project_root_path)\n",
        "    print(f\"'{project_root_path}' Í≤ΩÎ°úÍ∞Ä sys.pathÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§.\")\n",
        "else:\n",
        "    print(f\"'{project_root_path}' Í≤ΩÎ°úÎäî Ïù¥ÎØ∏ sys.pathÏóê ÏûàÏäµÎãàÎã§.\")\n",
        "\n",
        "# Ï∂îÍ∞ÄÎêú Í≤ΩÎ°ú ÌôïÏù∏ (ÏÑ†ÌÉù ÏÇ¨Ìï≠)\n",
        "print(\"Current sys.path:\")\n",
        "for p in sys.path:\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "umIL8VLsdR0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e5166b-a70b-4ceb-b971-7f24225b5930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/VAE-LSTM/' Í≤ΩÎ°úÍ∞Ä sys.pathÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§.\n",
            "Current sys.path:\n",
            "/content\n",
            "/env/python\n",
            "/usr/lib/python312.zip\n",
            "/usr/lib/python3.12\n",
            "/usr/lib/python3.12/lib-dynload\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages\n",
            "/usr/lib/python3/dist-packages\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/extensions\n",
            "/root/.ipython\n",
            "/content/drive/My Drive/VAE_ANP/\n",
            "/tmp/tmp5sifxfl_\n",
            "/content/drive/My Drive/VAE-LSTM/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌõàÎ†®"
      ],
      "metadata": {
        "id": "HkPB_PvzdkiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "from models import VAELSTMModel\n",
        "from data_loader import DataGenerator\n",
        "from trainers import ModelTrainer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the training process.\n",
        "    \"\"\"\n",
        "    # --- Configuration Loading ---\n",
        "    config_path = '/content/drive/MyDrive/VAE-LSTM/NAB_config.json'\n",
        "\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = json.load(f)\n",
        "            print(\"Configuration loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Configuration file not found at {config_path}\")\n",
        "        return\n",
        "\n",
        "    # --- Define Data Paths ---\n",
        "    train_excel_path = config['train_sensor_data_path']\n",
        "    val_excel_path = config['val_sensor_data_path']\n",
        "    train_image_dirs = config['train_image_dirs']\n",
        "    val_image_dirs = config['val_image_dirs']\n",
        "\n",
        "    # Create the result directory if it doesn't exist\n",
        "    if not os.path.exists(config['result_dir']):\n",
        "        os.makedirs(config['result_dir'])\n",
        "\n",
        "    # --- Component Instantiation ---\n",
        "    try:\n",
        "        print(\"Creating data loaders...\")\n",
        "        train_data_loader = DataGenerator(config, excel_path=train_excel_path,\n",
        "                                          image_dirs=train_image_dirs)\n",
        "\n",
        "        # IMPORTANT: Use the scaler from the training data for the validation set\n",
        "        val_data_loader = DataGenerator(config, excel_path=val_excel_path,\n",
        "                                        image_dirs=val_image_dirs, scaler = train_data_loader.scaler)\n",
        "\n",
        "        print(\"Data loaders created.\")\n",
        "    except (FileNotFoundError, KeyError) as e:\n",
        "        print(f\"Error: A data file was not found. Please check paths in config. Details: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- Model, Trainer, and Training ---\n",
        "    input_dims = len(train_data_loader.feature_cols)\n",
        "    model = VAELSTMModel(config, input_dims)\n",
        "    print(\"Model created.\")\n",
        "\n",
        "    trainer = ModelTrainer(model, train_data_loader, val_data_loader, config)\n",
        "    print(\"Trainer created.\")\n",
        "\n",
        "    # --- Start Training ---\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # --- Save Model and Scaler ---\n",
        "    print(\"Saving model weights and scaler...\")\n",
        "    model_weights_path = os.path.join(config['result_dir'], 'best_model_vl.weights.h5') # Changed from .h5 to .weights.h5\n",
        "    scaler_path = os.path.join(config['result_dir'], 'scaler_vl.pkl')\n",
        "\n",
        "    # Build the model before saving weights\n",
        "    try:\n",
        "        dummy_batch = next(iter(train_data_loader.get_dataset()))\n",
        "        model(dummy_batch[:2])\n",
        "        model.save_weights(model_weights_path)\n",
        "        joblib.dump(train_data_loader.scaler, scaler_path)\n",
        "        print(f\"Model weights saved to {model_weights_path}\")\n",
        "        print(f\"Scaler saved to {scaler_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving the model or scaler: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "L_GO6VvUdlpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a6c07f-4bd2-4769-e709-752725566ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded successfully.\n",
            "Creating data loaders...\n",
            "Creating image path map (recursive search)...\n",
            "Found 38002 unique images in the provided directories.\n",
            "Reading and preprocessing excel file: /content/preproc_train_data_final.xlsx\n",
            "Applying 1st order difference (Delta) processing...\n",
            "Fitting a new StandardScaler on normal data (after diff).\n",
            "Applying StandardScaler to columns: 49\n",
            "Number of Skip Image: 0\n",
            "Successfully loaded 37662 sequences and images.\n",
            "\n",
            "Creating image path map (recursive search)...\n",
            "Found 6000 unique images in the provided directories.\n",
            "Reading and preprocessing excel file: /content/preproc_val_data_final.xlsx\n",
            "Applying 1st order difference (Delta) processing...\n",
            "Applying StandardScaler to columns: 49\n",
            "Number of Skip Image: 0\n",
            "Successfully loaded 5964 sequences and images.\n",
            "\n",
            "Data loaders created.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Model created.\n",
            "Trainer created.\n",
            "Starting training...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [06:06<00:00,  1.60it/s, loss=55.9218]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.44it/s, val_loss=50.9825]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 55.9218, Validation Loss: 50.9825\n",
            "‚úÖ Validation Loss improved from inf to 50.9825. Saving model...\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=54.4360]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.40it/s, val_loss=50.9528]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.4360, Validation Loss: 50.9528\n",
            "‚úÖ Validation Loss improved from 50.9825 to 50.9528. Saving model...\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=54.6821]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.47it/s, val_loss=50.9689]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.6821, Validation Loss: 50.9689\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=54.9078]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.46it/s, val_loss=50.9787]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.9078, Validation Loss: 50.9787\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 2/5)\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=54.7854]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.47it/s, val_loss=50.9679]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.7854, Validation Loss: 50.9679\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 3/5)\n",
            "Epoch 6/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=54.4951]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.44it/s, val_loss=50.9524]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.4951, Validation Loss: 50.9524\n",
            "‚úÖ Validation Loss improved from 50.9528 to 50.9524. Saving model...\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=54.3742]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.44it/s, val_loss=50.9385]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.3742, Validation Loss: 50.9385\n",
            "‚úÖ Validation Loss improved from 50.9524 to 50.9385. Saving model...\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:48<00:00,  1.69it/s, loss=54.1758]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.44it/s, val_loss=50.9275]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.1758, Validation Loss: 50.9275\n",
            "‚úÖ Validation Loss improved from 50.9385 to 50.9275. Saving model...\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=54.1126]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.9206]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 54.1126, Validation Loss: 50.9206\n",
            "‚úÖ Validation Loss improved from 50.9275 to 50.9206. Saving model...\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.9698]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.42it/s, val_loss=50.9132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.9698, Validation Loss: 50.9132\n",
            "‚úÖ Validation Loss improved from 50.9206 to 50.9132. Saving model...\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.9658]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.46it/s, val_loss=50.9120]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.9658, Validation Loss: 50.9120\n",
            "‚úÖ Validation Loss improved from 50.9132 to 50.9120. Saving model...\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.9600]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.46it/s, val_loss=50.9005]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.9600, Validation Loss: 50.9005\n",
            "‚úÖ Validation Loss improved from 50.9120 to 50.9005. Saving model...\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.8681]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.40it/s, val_loss=50.8959]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.8681, Validation Loss: 50.8959\n",
            "‚úÖ Validation Loss improved from 50.9005 to 50.8959. Saving model...\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:44<00:00,  1.71it/s, loss=53.8294]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8878]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.8294, Validation Loss: 50.8878\n",
            "‚úÖ Validation Loss improved from 50.8959 to 50.8878. Saving model...\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.7440]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.45it/s, val_loss=50.8823]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.7440, Validation Loss: 50.8823\n",
            "‚úÖ Validation Loss improved from 50.8878 to 50.8823. Saving model...\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.6900]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.46it/s, val_loss=50.8746]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.6900, Validation Loss: 50.8746\n",
            "‚úÖ Validation Loss improved from 50.8823 to 50.8746. Saving model...\n",
            "Epoch 17/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.6716]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8717]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.6716, Validation Loss: 50.8717\n",
            "‚úÖ Validation Loss improved from 50.8746 to 50.8717. Saving model...\n",
            "Epoch 18/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:49<00:00,  1.68it/s, loss=53.6796]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.41it/s, val_loss=50.8681]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.6796, Validation Loss: 50.8681\n",
            "‚úÖ Validation Loss improved from 50.8717 to 50.8681. Saving model...\n",
            "Epoch 19/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=53.6262]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8633]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.6262, Validation Loss: 50.8633\n",
            "‚úÖ Validation Loss improved from 50.8681 to 50.8633. Saving model...\n",
            "Epoch 20/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=53.6335]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.46it/s, val_loss=50.8673]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.6335, Validation Loss: 50.8673\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 21/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.69it/s, loss=53.6209]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.47it/s, val_loss=50.8660]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.6209, Validation Loss: 50.8660\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 2/5)\n",
            "Epoch 22/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.69it/s, loss=53.5795]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.45it/s, val_loss=50.8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5795, Validation Loss: 50.8582\n",
            "‚úÖ Validation Loss improved from 50.8633 to 50.8582. Saving model...\n",
            "Epoch 23/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5763]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.44it/s, val_loss=50.8604]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5763, Validation Loss: 50.8604\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 24/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=53.5763]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.44it/s, val_loss=50.8544]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5763, Validation Loss: 50.8544\n",
            "‚úÖ Validation Loss improved from 50.8582 to 50.8544. Saving model...\n",
            "Epoch 25/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.69it/s, loss=53.5529]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8551]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5529, Validation Loss: 50.8551\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 26/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5601]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8543]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5601, Validation Loss: 50.8543\n",
            "‚úÖ Validation Loss improved from 50.8544 to 50.8543. Saving model...\n",
            "Epoch 27/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5348]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.42it/s, val_loss=50.8554]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5348, Validation Loss: 50.8554\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 28/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5322]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.40it/s, val_loss=50.8513]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5322, Validation Loss: 50.8513\n",
            "‚úÖ Validation Loss improved from 50.8543 to 50.8513. Saving model...\n",
            "Epoch 29/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5234]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.41it/s, val_loss=50.8515]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5234, Validation Loss: 50.8515\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 30/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5219]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.42it/s, val_loss=50.8513]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5219, Validation Loss: 50.8513\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 2/5)\n",
            "Epoch 31/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=53.5096]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:39<00:00,  2.38it/s, val_loss=50.8498]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5096, Validation Loss: 50.8498\n",
            "‚úÖ Validation Loss improved from 50.8513 to 50.8498. Saving model...\n",
            "Epoch 32/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:49<00:00,  1.68it/s, loss=53.5091]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.41it/s, val_loss=50.8463]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5091, Validation Loss: 50.8463\n",
            "‚úÖ Validation Loss improved from 50.8498 to 50.8463. Saving model...\n",
            "Epoch 33/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.5015]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.41it/s, val_loss=50.8483]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5015, Validation Loss: 50.8483\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 1/5)\n",
            "Epoch 34/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.4968]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:37<00:00,  2.47it/s, val_loss=50.8479]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.4968, Validation Loss: 50.8479\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 2/5)\n",
            "Epoch 35/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:46<00:00,  1.70it/s, loss=53.5048]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.40it/s, val_loss=50.8478]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.5048, Validation Loss: 50.8478\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 3/5)\n",
            "Epoch 36/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:47<00:00,  1.69it/s, loss=53.4926]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8466]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.4926, Validation Loss: 50.8466\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 4/5)\n",
            "Epoch 37/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 588/589 [05:45<00:00,  1.70it/s, loss=53.4946]\n",
            "Validating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 93/94 [00:38<00:00,  2.43it/s, val_loss=50.8471]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 53.4946, Validation Loss: 50.8471\n",
            "‚ö†Ô∏è Validation Loss did not improve. (Wait: 5/5)\n",
            "\n",
            "üõë Early Stopping Triggered! Validation loss did not improve for 5 consecutive epochs.\n",
            "Training stopped at Epoch 37.\n",
            "Training finished.\n",
            "Saving model weights and scaler...\n",
            "Model weights saved to /content/drive/MyDrive/Result_VL/best_model_vl.weights.h5\n",
            "Scaler saved to /content/drive/MyDrive/Result_VL/scaler_vl.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ÌÖåÏä§Ìä∏"
      ],
      "metadata": {
        "id": "Ul28iaksf0NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# ÏÇ¨Ïö©Ïûê Ï†ïÏùò Î™®Îìà (Í≤ΩÎ°úÎäî ÌôòÍ≤ΩÏóê ÎßûÍ≤å ÏàòÏ†ï ÌïÑÏöî)\n",
        "from models import VAELSTMModel\n",
        "from data_loader import DataGenerator\n",
        "\n",
        "# ====================================================\n",
        "# 1. Feature Extraction Function (Modified for tf.data.Dataset)\n",
        "# ====================================================\n",
        "def extract_vae_features(model, data_loader):\n",
        "    print(\"üöÄ [VAE-LSTM] Extracting features from Test Data (Sensor Only)...\")\n",
        "\n",
        "    results = {\n",
        "        'i2s_mse': [],      # Sensor Reconstruction Error\n",
        "        'i2s_max_error': [],# Sensor Max Reconstruction Error\n",
        "        's2i_mse': [],      # [Dummy] Image Reconstruction Error (0ÏúºÎ°ú Ï±ÑÏõÄ)\n",
        "        'sigma': [],        # Latent Uncertainty\n",
        "        'sensor_std': [],   # Input Sensor Std\n",
        "        'max_z_score': [],  # Input Sensor Max Z\n",
        "        'file_num': []      # For GT mapping\n",
        "    }\n",
        "\n",
        "    tf_dataset = data_loader.get_dataset()\n",
        "\n",
        "    try:\n",
        "        total_batches = len(data_loader.sequences) // data_loader.config['batch_size']\n",
        "    except:\n",
        "        total_batches = None\n",
        "\n",
        "    for i, batch_data in tqdm(enumerate(tf_dataset), total=total_batches, desc=\"Processing Batches\"):\n",
        "        sensor_in, img_in, _, _ = batch_data\n",
        "\n",
        "        # Î™®Îç∏ Ïã§Ìñâ (Îã®Ïùº Ï∂úÎ†•: ÏÑºÏÑú Ïû¨Íµ¨ÏÑ±)\n",
        "        recon_sensor = model([sensor_in, img_in])\n",
        "\n",
        "        # [Ï§ëÏöî] Î™®Îç∏Ïù¥ Î¶¨Ïä§Ìä∏Í∞Ä ÏïÑÎãàÎùº ÌÖêÏÑú ÌïòÎÇòÎßå Î∞òÌôòÌïúÎã§Í≥† Í∞ÄÏ†ï\n",
        "        if isinstance(recon_sensor, list):\n",
        "             recon_sensor = recon_sensor[0]\n",
        "\n",
        "        # 1. Sensor Reconstruction Error Í≥ÑÏÇ∞\n",
        "        sensor_diff = np.square(sensor_in.numpy() - recon_sensor.numpy())\n",
        "        i2s_mse_batch = np.mean(sensor_diff, axis=(1, 2))\n",
        "        i2s_max_batch = np.max(sensor_diff, axis=(1, 2))\n",
        "\n",
        "        # 2. Image Reconstruction Error (Í≥ÑÏÇ∞ Î∂àÍ∞Ä -> 0ÏúºÎ°ú Ï±ÑÏõÄ)\n",
        "        # MobileNetV2Îäî Ïù∏ÏΩîÎçîÏù¥ÎØÄÎ°ú Ïù¥ÎØ∏ÏßÄÎ•º Î≥µÏõêÌïòÏßÄ ÏïäÏùå\n",
        "        s2i_mse_batch = np.zeros(len(sensor_in))\n",
        "\n",
        "        # 3. Í∏∞ÌÉÄ ÌÜµÍ≥ÑÎüâ\n",
        "        if hasattr(model, 'code_std_dev'):\n",
        "            sigma_batch = np.mean(model.code_std_dev.numpy(), axis=1)\n",
        "        else:\n",
        "            sigma_batch = np.zeros(len(sensor_in))\n",
        "\n",
        "        sensor_std_batch = np.std(sensor_in.numpy(), axis=(1, 2))\n",
        "        max_z_batch = np.max(np.abs(sensor_in.numpy()), axis=(1, 2))\n",
        "\n",
        "        # Í≤∞Í≥º Ï†ÄÏû•\n",
        "        results['i2s_mse'].extend(i2s_mse_batch)\n",
        "        results['i2s_max_error'].extend(i2s_max_batch)\n",
        "        results['s2i_mse'].extend(s2i_mse_batch) # 0 Ï†ÄÏû•\n",
        "        results['sigma'].extend(sigma_batch)\n",
        "        results['sensor_std'].extend(sensor_std_batch)\n",
        "        results['max_z_score'].extend(max_z_batch)\n",
        "\n",
        "        current_batch_size = len(sensor_in)\n",
        "        current_count = len(results['file_num'])\n",
        "        results['file_num'].extend(range(current_count + 1, current_count + current_batch_size + 1))\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ====================================================\n",
        "# 2. Main Execution\n",
        "# ====================================================\n",
        "def main():\n",
        "    # --- A. Configuration & Setup ---\n",
        "    config_path = '/content/drive/MyDrive/VAE-LSTM/NAB_config.json'\n",
        "    try:\n",
        "        with open(config_path, 'r') as f: config = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Config file not found.\"); return\n",
        "\n",
        "    result_dir = config['result_dir']\n",
        "    scaler_path = os.path.join(result_dir, 'scaler_vl.pkl')\n",
        "    weights_path = os.path.join(result_dir, 'best_model_vl.weights.h5')\n",
        "    csv_path = \"vae_lstm_final_results.csv\"\n",
        "\n",
        "    # --- B. Load Model & Data ---\n",
        "    print(\"‚è≥ Loading Scaler and Model...\")\n",
        "    try:\n",
        "        scaler = joblib.load(scaler_path)\n",
        "    except:\n",
        "        print(\"‚ùå Scaler not found. Train the model first!\"); return\n",
        "\n",
        "    test_excel = config.get('test_sensor_data_path', config['val_sensor_data_path'])\n",
        "    test_img_dirs = config.get('test_image_dirs', config['val_image_dirs'])\n",
        "\n",
        "    # [ÏàòÏ†ï] shuffle Ïù∏Ïûê Ï†úÍ±∞ (DataGenerator __init__Ïóê ÏóÜÏùå)\n",
        "    test_loader = DataGenerator(config, excel_path=test_excel,\n",
        "                                image_dirs=test_img_dirs,\n",
        "                                scaler=scaler)\n",
        "\n",
        "    input_dims = len(test_loader.feature_cols)\n",
        "    model = VAELSTMModel(config, input_dims)\n",
        "\n",
        "    print(\"‚è≥ Building model with dummy data...\")\n",
        "    try:\n",
        "        # [ÏàòÏ†ï ÌïµÏã¨ 3] get_dataset()ÏùÑ Ìò∏Ï∂úÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Í∞ÄÏ†∏Ïò® Îí§, take(1)Î°ú 1Î∞∞ÏπòÎßå Í∞ÄÏ†∏Ïò¥\n",
        "        tf_dataset = test_loader.get_dataset()\n",
        "\n",
        "        # take(1)ÏùÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú Î∞∞Ïπò ÌïòÎÇòÎßå Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
        "        for batch in tf_dataset.take(1):\n",
        "            sensor_in, img_in, _, _ = batch # (sensor, image, len, label) Ïñ∏Ìå®ÌÇπ\n",
        "            model([sensor_in, img_in])      # Î™®Îç∏ ÎπåÎìú (Build)\n",
        "            break                           # Ìïú Î≤àÎßå ÌïòÍ≥† ÌÉàÏ∂ú\n",
        "\n",
        "        model.load_weights(weights_path)\n",
        "        print(\"‚úÖ Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to build model or load weights: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # --- C. Extract Features & Add GT & Save ---\n",
        "    # 1. ÌîºÏ≤ò Ï∂îÏ∂ú\n",
        "    df = extract_vae_features(model, test_loader)\n",
        "\n",
        "    # 2. GT(Ï†ïÎãµ) Î≥µÏÇ¨\n",
        "    try:\n",
        "        cvae_res = pd.read_csv(\"cross_modal_results.csv\")\n",
        "        # Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÍ∞Ä Îã§Î•º Ïàò ÏûàÏúºÎØÄÎ°ú Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ ÏûêÎ•¥Í±∞ÎÇò Í≤ΩÍ≥† Ï∂úÎ†•\n",
        "        if len(df) <= len(cvae_res):\n",
        "            df['gt'] = cvae_res['gt'][:len(df)]\n",
        "            print(\"‚úÖ Copied GT labels from CVAE-ANP results.\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Length mismatch (CVAE:{len(cvae_res)} vs VAE:{len(df)}). Cannot copy GT safely.\")\n",
        "            df['gt'] = 0\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è 'cross_modal_results.csv' not found. Cannot load GT.\")\n",
        "        return\n",
        "\n",
        "    # 3. CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úÖ Features saved to '{csv_path}'\")\n",
        "\n",
        "    # --- D. XGBoost Training & Grid Search ---\n",
        "    print(\"\\nüöÄ Starting XGBoost Grid Search...\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # [Ï∂îÍ∞Ä] Ïó¨Í∏∞ÏÑúÎ∂ÄÌÑ∞Í∞Ä ÏÉùÎûµÎêòÏóàÎçò XGBoost ÌïµÏã¨ Î°úÏßÅÏûÖÎãàÎã§.\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    # 1. ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ (Î°úÍ∑∏ Î≥ÄÌôò Î∞è ÌååÏÉù Î≥ÄÏàò ÏÉùÏÑ±)\n",
        "    # i2s_mse(ÏÑºÏÑú Î≥µÏõê Ïò§Ï∞®)Îäî Ï§ëÏöîÌïòÏßÄÎßå, s2i_mse(Ïù¥ÎØ∏ÏßÄ Î≥µÏõê Ïò§Ï∞®)Îäî 0Ïù¥ÎØÄÎ°ú ÏùòÎØ∏ ÏóÜÏùå\n",
        "    # ÌïòÏßÄÎßå CVAE-ANPÏôÄÏùò ÌòïÌèâÏÑ±ÏùÑ ÏúÑÌï¥ ÎèôÏùºÌïú Ïª¨Îüº Íµ¨Ï°∞Î•º Ïú†ÏßÄÌï©ÎãàÎã§.\n",
        "    df['log_i2s'] = np.log1p(df['i2s_mse'])\n",
        "    df['log_s2i'] = np.log1p(df['s2i_mse']) # 0 -> 0\n",
        "    df['log_i2s_max'] = np.log1p(df['i2s_max_error'])\n",
        "\n",
        "    df['diff_error'] = df['log_i2s'] - df['log_s2i']\n",
        "    df['ratio_error'] = df['i2s_mse'] / (df['s2i_mse'] + 1e-6)\n",
        "\n",
        "    features = [\n",
        "        'log_i2s', 'log_s2i', 'log_i2s_max',\n",
        "        'diff_error', 'ratio_error',\n",
        "        'max_z_score', 'sigma', 'sensor_std'\n",
        "    ]\n",
        "\n",
        "    # Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨ (0ÏúºÎ°ú Ï±ÑÏõÄ)\n",
        "    X = df[features].fillna(0)\n",
        "    y = df['gt'] # Ground Truth\n",
        "\n",
        "    # 2. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† (Tuning 50% / Evaluation 50%)\n",
        "    # stratify=y ÏòµÏÖòÏúºÎ°ú ÌÅ¥ÎûòÏä§ ÎπÑÏú®ÏùÑ Ïú†ÏßÄÌïòÎ©∞ ÎÇòÎàî\n",
        "    try:\n",
        "        X_tune, X_eval, y_tune, y_eval = train_test_split(\n",
        "            X, y, test_size=0.5, random_state=42, stratify=y\n",
        "        )\n",
        "        print(f\"‚úÇÔ∏è Data Split: Tuning={len(X_tune)}, Evaluation={len(X_eval)}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"‚ùå Error during split: {e}\")\n",
        "        print(\"Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÍ∞Ä ÎÑàÎ¨¥ Ï†ÅÍ±∞ÎÇò ÌÅ¥ÎûòÏä§ Î∂àÍ∑†ÌòïÏù¥ Ïã¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    # 3. Grid Search ÏÑ§Ï†ï\n",
        "    param_grid = {\n",
        "        'n_estimators': [200, 300],\n",
        "        'max_depth': [4, 6, 8, 10],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'colsample_bytree': [0.8, 1.0],\n",
        "        'gamma': [0, 0.1]\n",
        "    }\n",
        "\n",
        "    xgb = XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=4,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    print(\"üîç [VAE-LSTM] Starting Grid Search...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=xgb,\n",
        "        param_grid=param_grid,\n",
        "        cv=3,\n",
        "        scoring='accuracy',\n",
        "        verbose=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_tune, y_tune)\n",
        "\n",
        "    print(f\"\\nüèÜ Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"   Best CV Score (Tuning Set): {grid_search.best_score_:.2%}\")\n",
        "\n",
        "    # 4. ÏµúÏ¢Ö ÌèâÍ∞Ä (Evaluation Set)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_eval)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"       [VAE-LSTM] Final XGBoost Evaluation Report\")\n",
        "    print(\"=\"*60)\n",
        "    # digits=4Î•º Ï£ºÏñ¥ ÏÜåÏàòÏ†ê 4ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•\n",
        "    print(classification_report(y_eval, y_pred, target_names=['Normal', 'Case 1', 'Case 2', 'Case 3'], digits=4))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_eval, y_pred))\n",
        "\n",
        "    acc = accuracy_score(y_eval, y_pred)\n",
        "    print(f\"\\nüî• Final Accuracy: {acc:.2%}\")\n",
        "\n",
        "    # 5. Í≤∞Í≥º ÎπÑÍµê Î©òÌä∏ (CVAE-ANP Ï†êÏàòÏôÄ ÎπÑÍµê)\n",
        "    try:\n",
        "        # Ïù¥ Í∞íÏùÄ CVAE-ANP Ïã§ÌóòÏóêÏÑú ÎÇòÏò® Ï†ïÌôïÎèÑÎ•º ÏßÅÏ†ë Ï†ÅÏñ¥Ï£ºÍ±∞ÎÇò Î°úÎìúÌï¥Ïïº Ìï©ÎãàÎã§.\n",
        "        cvae_acc = 0.8807\n",
        "\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "        print(\"üìä [Comparison Summary]\")\n",
        "        print(f\"   CVAE-ANP Accuracy: {cvae_acc:.2%}\")\n",
        "        print(f\"   VAE-LSTM Accuracy: {acc:.2%}\")\n",
        "\n",
        "        if cvae_acc > acc:\n",
        "            diff = cvae_acc - acc\n",
        "            print(f\"‚úÖ Proposed model (CVAE-ANP) outperforms VAE-LSTM by {diff*100:.2f}%p!\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è VAE-LSTM is comparable or better.\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "CyIBibJdfyvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50171cd-25ea-4ca1-cf94-82c927c662ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Loading Scaler and Model...\n",
            "Creating image path map (recursive search)...\n",
            "Found 6000 unique images in the provided directories.\n",
            "Reading and preprocessing excel file: /content/preproc_test_data_final.xlsx\n",
            "Applying 1st order difference (Delta) processing...\n",
            "Applying StandardScaler to columns: 49\n",
            "Number of Skip Image: 0\n",
            "Successfully loaded 5804 sequences and images.\n",
            "\n",
            "‚è≥ Building model with dummy data...\n",
            "‚úÖ Model loaded successfully.\n",
            "üöÄ [VAE-LSTM] Extracting features from Test Data (Sensor Only)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:25<00:00,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Copied GT labels from CVAE-ANP results.\n",
            "‚úÖ Features saved to 'vae_lstm_final_results.csv'\n",
            "\n",
            "üöÄ Starting XGBoost Grid Search...\n",
            "‚úÇÔ∏è Data Split: Tuning=2880, Evaluation=2880\n",
            "üîç [VAE-LSTM] Starting Grid Search...\n",
            "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ Best Parameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300}\n",
            "   Best CV Score (Tuning Set): 67.92%\n",
            "\n",
            "============================================================\n",
            "       [VAE-LSTM] Final XGBoost Evaluation Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal     0.7776    0.8647    0.8188      1500\n",
            "      Case 1     0.6417    0.5660    0.6015       500\n",
            "      Case 2     0.6265    0.5200    0.5683       500\n",
            "      Case 3     0.6011    0.5632    0.5815       380\n",
            "\n",
            "    accuracy                         0.7132      2880\n",
            "   macro avg     0.6617    0.6285    0.6425      2880\n",
            "weighted avg     0.7045    0.7132    0.7063      2880\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1297   43  118   42]\n",
            " [  95  283   28   94]\n",
            " [ 211   23  260    6]\n",
            " [  65   92    9  214]]\n",
            "\n",
            "üî• Final Accuracy: 71.32%\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìä [Comparison Summary]\n",
            "   CVAE-ANP Accuracy: 88.07%\n",
            "   VAE-LSTM Accuracy: 71.32%\n",
            "‚úÖ Proposed model (CVAE-ANP) outperforms VAE-LSTM by 16.75%p!\n"
          ]
        }
      ]
    }
  ]
}